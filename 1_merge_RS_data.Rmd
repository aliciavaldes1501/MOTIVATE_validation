---
title: "Script to merge all RS data"
author: "Alicia Vald√©s"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: html_notebook
---

This R script is used to merge RS data downloaded the Google Drive folder shared by Bea with the ReSurvey database.

# Load libraries

```{r}
library(tidyverse)
library(here)
```

# Read files with RS data from Bea

## S2 

### NDVI & NDMI

### NOTE: To be updated with files in folders "New_filter"

```{r}
# Set the folder path
folder_path <- "C:/Data/MOTIVATE/MOTIVATE_RS_data/S2"

# List only CSV files that contain "max_Filtered" in their filename
csv_files <- list.files(folder_path, pattern = "max_Filtered.*\\.csv$",
                        full.names = TRUE, recursive = TRUE)

# Function to read each file and extract info from the filename
read_and_label <- function(file_path) {
  file_name <- basename(file_path)
  
  # Extract region and subregion from the filename
  # Updated regular expression to handle the case where subregion is missing
  components <- str_match(file_name,
                          "^[0-9_]*([^_]+)_([^_]+)_Sentinel.*?_(.*?).csv")
  
  # Extract the biogeo and unit, handling missing subregion (unit)
  biogeo <- components[2]
  
  # If subregion (unit) is missing, set it as NA
  unit <- ifelse(is.na(components[3]), NA, components[3])
  
  # Check if biogeo is missing, and if so,
  # assign the first part of the filename (region name)
  if (is.na(biogeo) && grepl("Sentinel", file_name)) {
    # Capture the first part (biogeo) directly
    biogeo <- str_match(file_name, "^[0-9_]*([^_]+)_Sentinel")[2]
  }
  
  # If biogeo is still NA, print a warning
  if (is.na(biogeo)) {
    warning(paste("Failed to extract biogeo for file:", file_name))
  }

  # Read CSV and add columns for extracted info
  read_csv(file_path) %>%
    mutate(biogeo = biogeo, unit = unit)
}
# Read and merge all CSV files
data_RS_S2 <- map_dfr(csv_files, read_and_label)

# View the resulting tibble
print(data_RS_S2)

# Counts per biogeo and unit
print(data_RS_S2 %>% count(biogeo, unit), n = 100)
```

#### Data cleaning S2 NDVI & NDMI

```{r}
data_RS_S2 <- data_RS_S2 %>%
  # Keep the columns we need
  select(obs_unique, biogeo, unit, year, source, Lat_update, Lon_update,
         NDVI, NDMI) %>%
  # Rename Lat and Lon, these are only kept in case there is difference with
  # those in the ReSurvey database due to updates based on Ilona's info
  rename(Lat_RS = Lat_update, Lon_RS = Lon_update) %>%
  # Same for year
  rename(year_RS = year)
```

### SOS, Peak and EOS

```{r}
# Set the folder path
folder_path <- "C:/Data/MOTIVATE/MOTIVATE_RS_data/S2"

# List only CSV files that contain "NDVI_Phenology" in their filename
csv_files <- list.files(folder_path, pattern = "NDVI_Phenology.*\\.csv$",
                        full.names = TRUE, recursive = TRUE)

# Function to read each file and extract info from the filename
read_and_label <- function(file_path) {
  file_name <- basename(file_path)
  
  # Extract region and subregion from the filename
  # Updated regular expression to handle the case where subregion is missing
  components <- str_match(file_name,
                          "^[0-9_]*([^_]+)_([^_]+)_NDVI_Phenology.*?_(.*?).csv")
  
  # Extract the biogeo and unit, handling missing subregion (unit)
  biogeo <- components[2]
  
  # If subregion (unit) is missing, set it as NA
  unit <- ifelse(is.na(components[3]), NA, components[3])
  
  # Check if biogeo is missing, and if so,
  # assign the first part of the filename (region name)
  if (is.na(biogeo) && grepl("_NDVI_Phenology", file_name)) {
    # Capture the first part (biogeo) directly
    biogeo <- str_match(file_name, "^[0-9_]*([^_]+)_NDVI_Phenology")[2]
  }
  
  # If biogeo is still NA, print a warning
  if (is.na(biogeo)) {
    warning(paste("Failed to extract biogeo for file:", file_name))
  }

  # Read CSV and add columns for extracted info
  read_csv(file_path) %>%
    mutate(biogeo = biogeo, unit = unit)
}
# Read and merge all CSV files
data_RS_S2_phen <- map_dfr(csv_files, read_and_label)

# View the resulting tibble
print(data_RS_S2_phen)

# Counts per biogeo and unit
print(data_RS_S2_phen %>% count(biogeo, unit), n = 100)
```

#### Data cleaning S2 SOS, Peak and EOS

```{r}
data_RS_S2_phen <- data_RS_S2_phen %>%
  # Keep the columns we need
  select(obs_unique, biogeo, unit, SOS_DOY, Peak_DOY, EOS_DOY, EOS_Date)
  # Remove Lat and Lon and year, in case there is difference with
  # those in the ReSurvey database due to updates based on Ilona's info,
  # we have Lat_RS, Lon_RS and year_RS from data_RS_S2
```

## Landsat

### NDVI, NDMI & other stuff

### NOTE: some regions missing (at least CON)

```{r}
# Set the folder path
folder_path <- "C:/Data/MOTIVATE/MOTIVATE_RS_data/Landsat"

# List only CSV files that contain "Plot" in their filename
csv_files <- list.files(folder_path, pattern = "Plot.*\\.csv$",
                        full.names = TRUE, recursive = TRUE)

# Remove ALP_BAL so far cause there seems to be an error in that table

csv_files <- csv_files[-1] 

# Define the expected column names
expected_columns <- c("system:index", "Lat_update", "Lon_update", "obs_unique",
                      "plot_uniqu",	"source",	"year",	"EVI_max",	"EVI_median",
                      "EVI_min", "EVI_p10",	"EVI_p90",	"EVI_stdDev",	"EVImean",
                      "NDMI_max",	"NDMI_median",	"NDMI_min",	"NDMI_p10",
                      "NDMI_p90",	"NDMI_stdDev",	"NDMImean",	"NDVI_max",
                      "NDVI_median",	"NDVI_min",	"NDVI_p10",	"NDVI_p90",
                      "NDVI_stdDev",	"NDVImean",	"NDWI_max",	"NDWI_median",
                      "NDWI_min",	"NDWI_p10",	"NDWI_p90",	"NDWI_stdDev",
                      "NDWImean",	"SAVI_max",	"SAVI_median",	"SAVI_min",
                      "SAVI_p10",	"SAVI_p90",	"SAVI_stdDev",	"SAVImean",	".geo") 

# Define the column types
column_types <- cols(
  `system:index` = col_character(), Lat_update = col_double(),
  Lon_update = col_double(), obs_unique = col_double(),
  plot_uniqu = col_character(), source = col_character(), year = col_integer(),
  EVI_max = col_double(), EVI_median = col_double(), EVI_min = col_double(),
  EVI_p10 = col_double(), EVI_p90 = col_double(), EVI_stdDev = col_double(),
  EVImean = col_double(), NDMI_max = col_double(), NDMI_median = col_double(),
  NDMI_min = col_double(), NDMI_p10 = col_double(), NDMI_p90 = col_double(),
  NDMI_stdDev = col_double(), NDMImean = col_double(), NDVI_max = col_double(),
  NDVI_median = col_double(), NDVI_min = col_double(), NDVI_p10 = col_double(),
  NDVI_p90 = col_double(), NDVI_stdDev = col_double(), NDVImean = col_double(),
  NDWI_max = col_double(), NDWI_median = col_double(), NDWI_min = col_double(),
  NDWI_p10 = col_double(), NDWI_p90 = col_double(), NDWI_stdDev = col_double(),
  NDWImean = col_double(), SAVI_max = col_double(), SAVI_median = col_double(),
  SAVI_min = col_double(), SAVI_p10 = col_double(), SAVI_p90 = col_double(),
  SAVI_stdDev = col_double(), SAVImean = col_double(), .geo = col_character()
)

# Function to read each file and extract info from the filename
read_and_label <- function(file_path) {
  file_name <- basename(file_path)
  
  # Extract region and subregion from the filename
  # Updated regular expression to handle the case where subregion is missing
  components <- str_match(file_name,
                          "^[0-9_]*([^_]+)_([^_]+)_Landsat.*?_(.*?).csv")
  
  # Extract the biogeo and unit, handling missing subregion (unit)
  biogeo <- components[2]
  
  # If subregion (unit) is missing, set it as NA
  unit <- ifelse(is.na(components[3]), NA, components[3])
  
  # Check if biogeo is missing, and if so,
  # assign the first part of the filename (region name)
  if (is.na(biogeo) && grepl("Landsat", file_name)) {
    # Capture the first part (biogeo) directly
    biogeo <- str_match(file_name, "^[0-9_]*([^_]+)_Landsat")[2]
  }
  
  # If biogeo is still NA, print a warning
  if (is.na(biogeo)) {
    warning(paste("Failed to extract biogeo for file:", file_name))
  }
  
  delimiter <- ifelse(grepl(";", readLines(file_path, n = 1)), ";", ",")
  
  # Read CSV and add columns for extracted info
  data <- read_delim(file_path, delim = delimiter, col_types = column_types) %>%
    mutate(biogeo = biogeo, unit = unit)
  
  # Reorder columns based on expected columns
  data <- data %>%
    select(all_of(expected_columns), everything())
  
  return(data)
}

# Read and merge all CSV files
data_RS_Landsat <- map_dfr(csv_files, read_and_label)

# View the resulting tibble
print(data_RS_Landsat)

# Counts per biogeo and unit
print(data_RS_Landsat %>% count(biogeo, unit), n = 100)
```

#### Data cleaning Landsat NDVI & NDMI

```{r}
data_RS_Landsat <- data_RS_Landsat %>%
  # Keep the columns we need
  select(obs_unique, biogeo, unit, year, source, Lat_update, Lon_update,
         NDVI_max, NDMI_max) %>% # Keep only these two so far
  # Rename those as NDVI and NDMI to agree with S2 data
  rename(NDVI = NDVI_max, NDMI = NDMI_max) %>%
  # Rename Lat and Lon, these are only kept in case there is differrence with
  # those in the ReSurvey database due to updates based on Ilona's info
  rename(Lat_RS = Lat_update, Lon_RS = Lon_update) %>%
  # Same for year
  rename(year_RS = year)
```

## Canopy height

```{r}
data_RS_CH <- read_csv(
  "C:/Data/MOTIVATE/MOTIVATE_RS_data/Canopy_Height_1m/Europe_points_CanopyHeight_1m.csv")
data_RS_CH
```

### Data cleaning CH

```{r}
data_RS_CH <- data_RS_CH %>%
  # Keep the columns we need
  select(obs_unique, canopy_height)
```

## TBD: Biomass

# Read file db_Europa

In this file, there is the correspondence obs_unique - PlotObservationID.

```{r}
db_Europa <- read_csv(
  here("..", "DB_first_check", "data", "clean","db_Europa_20250107.csv")
  )
```

Get only the columns PlotObservationID (original unique identifier) obs_unique_id (unique identified created by me) and year.

```{r}
db_Europa <- db_Europa %>% select(PlotObservationID, obs_unique_id)
```

# Merge RS data and db_Europa

```{r}
data_RS_S2_ID <- db_Europa %>%
  right_join(data_RS_S2 %>%
              # Rename to be able to join on this column
              rename(obs_unique_id = obs_unique))
```

Now we have PlotObservationID in data_RS_S2_ID.

```{r}
data_RS_S2_phen_ID <- db_Europa %>%
  right_join(data_RS_S2_phen %>%
              # Rename to be able to join on this column
              rename(obs_unique_id = obs_unique))

```

Now we have PlotObservationID in data_RS_S2_phen_ID

```{r}
data_RS_Landsat_ID <- db_Europa %>%
  right_join(data_RS_Landsat %>%
              # Rename to be able to join on this column
              rename(obs_unique_id = obs_unique))
```

Now we have PlotObservationID in data_RS_Landsat_ID.

```{r}
data_RS_CH_ID <- db_Europa %>%
  right_join(data_RS_CH %>%
              # Rename to be able to join on this column
              rename(obs_unique_id = obs_unique))
```

Now we have PlotObservationID in data_RS_CH_ID.

# Read file db_resurv_updated_clean

This is the ReSurvey database after updates (to be continued).

```{r}
db_resurv <- read_tsv(
  here("..", "DB_first_check","data", "clean","db_resurv_updated_clean.csv"),
  col_types = cols(
    # Dynamically specify EUNIS columns as character
    .default = col_guess(),  # Default guessing for other columns
    EUNISa = col_character(),
    EUNISb = col_character(),
    EUNISc = col_character(),
    EUNISd = col_character(),
    EUNISa_1 = col_character(),
    EUNISa_2 = col_character(),
    EUNISa_3 = col_character(),
    EUNISa_4 = col_character(),
    EUNISb_1 = col_character(),
    EUNISb_2 = col_character(),
    EUNISb_3 = col_character(),
    EUNISb_4 = col_character(),
    EUNISc_1 = col_character(),
    EUNISc_2 = col_character(),
    EUNISc_3 = col_character(),
    EUNISc_4 = col_character(),
    EUNISd_1 = col_character(),
    EUNISd_2 = col_character(),
    EUNISd_3 = col_character(),
    EUNISd_4 = col_character(),
    EUNISa_1_descr = col_character(),
    EUNISb_1_descr = col_character(),
    EUNISc_1_descr = col_character(),
    EUNISd_1_descr = col_character(),
    EUNIS_assignation = col_character(),
    EUNISa_2_descr = col_character(),
    EUNISa_3_descr = col_character(),
    EUNISa_4_descr = col_character(),
    EUNISb_2_descr = col_character(),
    EUNISb_3_descr = col_character(),
    EUNISb_4_descr = col_character(),
    EUNISc_2_descr = col_character(),
    EUNISc_3_descr = col_character(),
    EUNISc_4_descr = col_character(),
    EUNISd_2_descr = col_character(),
    EUNISd_3_descr = col_character(),
    EUNISd_4_descr = col_character()
    )
  )
```

No parsing issues!

# Merge RS data to the ReSurvey database

For some points, there is data (NDVI and NDMI so far) both from S2 and Landsat. In those cases, use the S2 data because it is more precise (10 m vs 30 m).

```{r}
data_RS_S2_ID <- data_RS_S2_ID %>%
  rename(NDVI_S2 = NDVI, NDMI_S2 = NDMI) %>%
  select(-source)
data_RS_Landsat_ID <- data_RS_Landsat_ID %>%
  rename(NDVI_Landsat = NDVI, NDMI_Landsat = NDMI) %>%
  select(-source)
```

Join S2, S2_phen and Landsat data:

```{r}
data_RS <- data_RS_S2_ID %>% 
  full_join(data_RS_S2_phen_ID) %>%
  full_join(data_RS_Landsat_ID)
```

Number of observations with NDVI data from both S2 and Landsat:

```{r}
nrow(data_RS %>% filter(!is.na(NDVI_S2) & !is.na(NDVI_Landsat)))
```

Send points in .csv to Bea.

```{r}
write.csv(data_RS %>% filter(!is.na(NDVI_S2) & !is.na(NDVI_Landsat)),
          file = here("data", "clean", "points_NDVI_S2_Landsat.csv"))
```

Difference between NDVI values from S2 and Landsat:

```{r}
data_RS %>% filter(!is.na(NDVI_S2) & !is.na(NDVI_Landsat)) %>%
  mutate(diff_NDVI = NDVI_S2 - NDVI_Landsat) %>%
  ggplot(aes(x = diff_NDVI)) + geom_histogram(color = "black", fill = "white")
data_RS %>% filter(!is.na(NDMI_S2) & !is.na(NDMI_Landsat)) %>%
  mutate(diff_NDMI = NDMI_S2 - NDMI_Landsat) %>%
  ggplot(aes(x = diff_NDMI)) + geom_histogram(color = "black", fill = "white")
```

There is a large difference between NDVI values from S2 and Landsat. So far, use the S2 data, but check with Bea.

When NDVI and NDMI values are available from both satellites, use S2:

```{r}
data_RS <- data_RS %>%
  mutate(NDVI =
           case_when(
             is.na(NDVI_S2) & is.na(NDVI_Landsat) ~ NA_real_,
             is.na(NDVI_Landsat) ~ NDVI_S2,
             is.na(NDVI_S2) ~ NDVI_Landsat,
             TRUE ~ NDVI_S2),
         NDMI = 
           case_when(
             is.na(NDMI_S2) & is.na(NDMI_Landsat) ~ NA_real_,
             is.na(NDMI_Landsat) ~ NDMI_S2,
             is.na(NDMI_S2) ~ NDMI_Landsat,
             TRUE ~ NDMI_S2),
         )
```

```{r}
db_resurv_RS <- db_resurv %>%
  left_join(data_RS %>% select(-obs_unique_id)) %>%
  left_join(data_RS_CH_ID %>% select(-obs_unique_id)) %>%
  mutate(S2_data = !is.na(NDVI_S2) & !is.na(NDMI_S2), 
         Landsat_data = !is.na(NDVI_Landsat) & !is.na(NDMI_Landsat),
         CH_data = !is.na(canopy_height),
         S2_phen_data = !is.na(SOS_DOY))
```

```{r}
db_resurv_RS %>% count(S2_data)
db_resurv_RS %>% count(Landsat_data)
db_resurv_RS %>% count(CH_data)
db_resurv_RS %>% count(S2_phen_data)
```

# Save to clean data

Save clean file for analyses (to be updated continuously due to updates in ReSurvey database and updates on RS data).

```{r}
write_tsv(db_resurv_RS,here("data", "clean","db_resurv_RS_20250331.csv"))
```

# Session info

```{r}
sessionInfo()
```
