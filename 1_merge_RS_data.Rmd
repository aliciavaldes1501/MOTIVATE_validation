---
title: "Script to merge all RS data"
author: "Alicia Vald√©s"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: html_notebook
---

This R script is used to merge RS data downloaded the Google Drive folder shared by Bea with the ReSurvey database.

# Load libraries

```{r}
library(tidyverse)
library(here)
library(lubridate)
```

# Read files with RS data from Bea

## S2 

### NDVI, NDMI & other indices

### NOTE: Updated with files in folders "New_filter" (ATL-BEN y CON_NOR different number, Bea is into this)
### NOTE: So far excluding ALP-BAL because csv file is wrong

```{r}
# Set the folder path
folder_path <- "C:/Data/MOTIVATE/MOTIVATE_RS_data/S2"

# List only CSV files that contain "max_Filtered" in their filename
csv_files <- list.files(folder_path,
                        pattern = "Sentinel_Mean_Plot_Year_Allmetrics.*\\.csv$",
                        full.names = TRUE, recursive = TRUE)
# So far remove this one that is wrong
csv_files <- setdiff(csv_files,
                     "C:/Data/MOTIVATE/MOTIVATE_RS_data/S2/ALP/New_filter/ALP_BAL_Sentinel_Mean_Plot_Year_Allmetrics.csv")

# Function to read each file and extract info from the filename
read_and_label <- function(file_path) {
  file_name <- basename(file_path)
  
  # Extract region and subregion from the filename
  # Updated regular expression to handle the case where subregion is missing
  components <- str_match(file_name,
                          "^[0-9_]*([^_]+)_([^_]+)_Sentinel.*?_(.*?).csv")
  
  # Extract the biogeo and unit, handling missing subregion (unit)
  biogeo <- components[2]
  
  # If subregion (unit) is missing, set it as NA
  unit <- ifelse(is.na(components[3]), NA, components[3])
  
  # Check if biogeo is missing, and if so,
  # assign the first part of the filename (region name)
  if (is.na(biogeo) && grepl("Sentinel", file_name)) {
    # Capture the first part (biogeo) directly
    biogeo <- str_match(file_name, "^[0-9_]*([^_]+)_Sentinel")[2]
  }
  
  # If biogeo is still NA, print a warning
  if (is.na(biogeo)) {
    warning(paste("Failed to extract biogeo for file:", file_name))
  }

  # Read CSV and add columns for extracted info
  read_csv(file_path) %>%
    mutate(biogeo = biogeo, unit = unit)
}
# Read and merge all CSV files
data_RS_S2 <- map_dfr(csv_files, read_and_label)

# View the resulting tibble
print(data_RS_S2)

# Counts per biogeo and unit
print(data_RS_S2 %>% count(biogeo, unit), n = 100)
```

#### Data cleaning

Keep all indices and metrics in case they are useful.

```{r}
data_RS_S2 <- data_RS_S2 %>%
  # Keep the columns we need
  select(obs_unique, biogeo, unit, year, source, Lat_update, Lon_update,
         starts_with("NDVI"), starts_with("NDMI"), starts_with("NDWI"),
         starts_with("EVI"), starts_with("SAVI")) %>%
  # Rename Lat and Lon, these are only kept in case there is difference with
  # those in the ReSurvey database due to updates based on Ilona's info
  rename(Lat_RS = Lat_update, Lon_RS = Lon_update) %>%
  # Same for year
  rename(year_RS = year)
```

### Phenology S2

Manually renamed all files as REGION_SUBREGION_Phenology.

```{r}
# Set the folder path
folder_path <- "C:/Data/MOTIVATE/MOTIVATE_RS_data/S2"

# List only CSV files that contain "Phenology" in their filename
csv_files <- list.files(folder_path, pattern = "Phenology.*\\.csv$",
                        full.names = TRUE, recursive = TRUE)

# Function to read each file and extract info from the filename
read_and_label <- function(file_path) {
  file_name <- basename(file_path)
  
  # Extract region and subregion from the filename
  # Updated regular expression to handle the case where subregion is missing
  components <- str_match(file_name,
                          "^[0-9_]*([^_]+)_([^_]+)_Phenology.csv")
  
  # Extract the biogeo and unit, handling missing subregion (unit)
  biogeo <- components[2]
  
  # If subregion (unit) is missing, set it as NA
  unit <- ifelse(is.na(components[3]), NA, components[3])
  
  # Check if biogeo is missing, and if so,
  # assign the first part of the filename (region name)
  if (is.na(biogeo) && grepl("_Phenology", file_name)) {
    # Capture the first part (biogeo) directly
    biogeo <- str_match(file_name, "^[0-9_]*([^_]+)_Phenology")[2]
  }
  
  # If biogeo is still NA, print a warning
  if (is.na(biogeo)) {
    warning(paste("Failed to extract biogeo for file:", file_name))
  }

  # Read CSV and add columns for extracted info
  read_csv(file_path) %>%
    mutate(biogeo = biogeo, unit = unit)
}
# Read and merge all CSV files
data_RS_S2_phen <- map_dfr(csv_files, read_and_label)

# View the resulting tibble
print(data_RS_S2_phen)

# Counts per biogeo and unit
print(data_RS_S2_phen %>% count(biogeo, unit), n = 100)
```

#### Data cleaning

```{r}
data_RS_S2_phen <- data_RS_S2_phen %>%
  # Keep the columns we need:
  # Dates were wrongly exported from GEE - removed them and recalculated later.
  # Remove Lat and Lon and year, in case there is difference with
  # those in the ReSurvey database due to updates based on Ilona's info,
  # we have Lat_RS, Lon_RS and year_RS from data_RS_S2
  select(obs_unique, biogeo, unit, SOS_DOY, NDVI_at_SOS, Peak_DOY, NDVI_at_Peak,
         EOS_DOY, NDVI_at_EOS, Season_Length, year) %>%
  # make_date() creates a date at the start of the year (January 1st). 
  # By adding days(DOY - 1), we correctly position the date within the year.
  mutate(SOS_date = make_date(year) + days(SOS_DOY - 1),
         Peak_date = make_date(year) + days(Peak_DOY - 1),
         EOS_date = make_date(year) + days(EOS_DOY - 1)) %>%
  # Remove year
  select(-year)
```

## Landsat

### HERE: Do not use so far cause metrics based only in July, not all year!
### NDVI, NDMI & other indices
### NOTE: some regions missing (at least CON)
### NOTE: So far excluding ALP-BAL because csv file is wrong

```{r}
# Set the folder path
folder_path <- "C:/Data/MOTIVATE/MOTIVATE_RS_data/Landsat"

# List only CSV files that contain "Plot" in their filename
csv_files <- list.files(folder_path, pattern = "Plot.*\\.csv$",
                        full.names = TRUE, recursive = TRUE)

# Remove ALP_BAL so far cause there seems to be an error in that table

csv_files <- csv_files[-1] 

# Define the expected column names
expected_columns <- c("system:index", "Lat_update", "Lon_update", "obs_unique",
                      "plot_uniqu",	"source",	"year",	"EVI_max",	"EVI_median",
                      "EVI_min", "EVI_p10",	"EVI_p90",	"EVI_stdDev",	"EVImean",
                      "NDMI_max",	"NDMI_median",	"NDMI_min",	"NDMI_p10",
                      "NDMI_p90",	"NDMI_stdDev",	"NDMImean",	"NDVI_max",
                      "NDVI_median",	"NDVI_min",	"NDVI_p10",	"NDVI_p90",
                      "NDVI_stdDev",	"NDVImean",	"NDWI_max",	"NDWI_median",
                      "NDWI_min",	"NDWI_p10",	"NDWI_p90",	"NDWI_stdDev",
                      "NDWImean",	"SAVI_max",	"SAVI_median",	"SAVI_min",
                      "SAVI_p10",	"SAVI_p90",	"SAVI_stdDev",	"SAVImean",	".geo") 

# Define the column types
column_types <- cols(
  `system:index` = col_character(), Lat_update = col_double(),
  Lon_update = col_double(), obs_unique = col_double(),
  plot_uniqu = col_character(), source = col_character(), year = col_integer(),
  EVI_max = col_double(), EVI_median = col_double(), EVI_min = col_double(),
  EVI_p10 = col_double(), EVI_p90 = col_double(), EVI_stdDev = col_double(),
  EVImean = col_double(), NDMI_max = col_double(), NDMI_median = col_double(),
  NDMI_min = col_double(), NDMI_p10 = col_double(), NDMI_p90 = col_double(),
  NDMI_stdDev = col_double(), NDMImean = col_double(), NDVI_max = col_double(),
  NDVI_median = col_double(), NDVI_min = col_double(), NDVI_p10 = col_double(),
  NDVI_p90 = col_double(), NDVI_stdDev = col_double(), NDVImean = col_double(),
  NDWI_max = col_double(), NDWI_median = col_double(), NDWI_min = col_double(),
  NDWI_p10 = col_double(), NDWI_p90 = col_double(), NDWI_stdDev = col_double(),
  NDWImean = col_double(), SAVI_max = col_double(), SAVI_median = col_double(),
  SAVI_min = col_double(), SAVI_p10 = col_double(), SAVI_p90 = col_double(),
  SAVI_stdDev = col_double(), SAVImean = col_double(), .geo = col_character()
)

# Function to read each file and extract info from the filename
read_and_label <- function(file_path) {
  file_name <- basename(file_path)
  
  # Extract region and subregion from the filename
  # Updated regular expression to handle the case where subregion is missing
  components <- str_match(file_name,
                          "^[0-9_]*([^_]+)_([^_]+)_Landsat.*?_(.*?).csv")
  
  # Extract the biogeo and unit, handling missing subregion (unit)
  biogeo <- components[2]
  
  # If subregion (unit) is missing, set it as NA
  unit <- ifelse(is.na(components[3]), NA, components[3])
  
  # Check if biogeo is missing, and if so,
  # assign the first part of the filename (region name)
  if (is.na(biogeo) && grepl("Landsat", file_name)) {
    # Capture the first part (biogeo) directly
    biogeo <- str_match(file_name, "^[0-9_]*([^_]+)_Landsat")[2]
  }
  
  # If biogeo is still NA, print a warning
  if (is.na(biogeo)) {
    warning(paste("Failed to extract biogeo for file:", file_name))
  }
  
  delimiter <- ifelse(grepl(";", readLines(file_path, n = 1)), ";", ",")
  
  # Read CSV and add columns for extracted info
  data <- read_delim(file_path, delim = delimiter, col_types = column_types) %>%
    mutate(biogeo = biogeo, unit = unit)
  
  # Reorder columns based on expected columns
  data <- data %>%
    select(all_of(expected_columns), everything())
  
  return(data)
}

# Read and merge all CSV files
data_RS_Landsat <- map_dfr(csv_files, read_and_label)

# View the resulting tibble
print(data_RS_Landsat)

# Counts per biogeo and unit
print(data_RS_Landsat %>% count(biogeo, unit), n = 100)
```

#### Data cleaning

Keep all indices and metrics in case they are useful.

```{r}
data_RS_Landsat <- data_RS_Landsat %>%
  # Keep the columns we need
  select(obs_unique, biogeo, unit, year, source, Lat_update, Lon_update,
         starts_with("NDVI"), starts_with("NDMI"), starts_with("NDWI"),
         starts_with("EVI"), starts_with("SAVI")) %>%
  # Rename Lat and Lon, these are only kept in case there is difference with
  # those in the ReSurvey database due to updates based on Ilona's info
  rename(Lat_RS = Lat_update, Lon_RS = Lon_update) %>%
  # Same for year
  rename(year_RS = year)
```

### TO ADD: Phenology Landsat

## Canopy height

```{r}
data_RS_CH <- read_csv(
  "C:/Data/MOTIVATE/MOTIVATE_RS_data/Canopy_Height_1m/Europe_points_CanopyHeight_1m.csv")
data_RS_CH
```

### Data cleaning CH

```{r}
data_RS_CH <- data_RS_CH %>%
  # Keep the columns we need
  select(obs_unique, canopy_height)
```

## TBD: Biomass

# Read file db_Europa

In this file, there is the correspondence obs_unique - PlotObservationID.

```{r}
db_Europa <- read_csv(
  here("..", "DB_first_check", "data", "clean","db_Europa_20250107.csv")
  )
```

Get only the columns PlotObservationID (original unique identifier) obs_unique_id (unique identified created by me) and year.

```{r}
db_Europa <- db_Europa %>% select(PlotObservationID, obs_unique_id)
```

# Merge RS data and db_Europa

```{r}
data_RS_S2_ID <- db_Europa %>%
  right_join(data_RS_S2 %>%
              # Rename to be able to join on this column
              rename(obs_unique_id = obs_unique))
```

Now we have PlotObservationID in data_RS_S2_ID.

```{r}
data_RS_S2_phen_ID <- db_Europa %>%
  right_join(data_RS_S2_phen %>%
              # Rename to be able to join on this column
              rename(obs_unique_id = obs_unique))

```

Now we have PlotObservationID in data_RS_S2_phen_ID

```{r}
data_RS_Landsat_ID <- db_Europa %>%
  right_join(data_RS_Landsat %>%
              # Rename to be able to join on this column
              rename(obs_unique_id = obs_unique))
```

Now we have PlotObservationID in data_RS_Landsat_ID.

```{r}
data_RS_CH_ID <- db_Europa %>%
  right_join(data_RS_CH %>%
              # Rename to be able to join on this column
              rename(obs_unique_id = obs_unique))
```

Now we have PlotObservationID in data_RS_CH_ID.

# Read file db_resurv_updated_clean

This is the ReSurvey database after updates (to be continued).

```{r}
db_resurv <- read_tsv(
  here("..", "DB_first_check","data", "clean","db_resurv_updated_clean.csv"),
  col_types = cols(
    # Dynamically specify EUNIS columns as character
    .default = col_guess(),  # Default guessing for other columns
    EUNISa = col_character(),
    EUNISb = col_character(),
    EUNISc = col_character(),
    EUNISd = col_character(),
    EUNISa_1 = col_character(),
    EUNISa_2 = col_character(),
    EUNISa_3 = col_character(),
    EUNISa_4 = col_character(),
    EUNISb_1 = col_character(),
    EUNISb_2 = col_character(),
    EUNISb_3 = col_character(),
    EUNISb_4 = col_character(),
    EUNISc_1 = col_character(),
    EUNISc_2 = col_character(),
    EUNISc_3 = col_character(),
    EUNISc_4 = col_character(),
    EUNISd_1 = col_character(),
    EUNISd_2 = col_character(),
    EUNISd_3 = col_character(),
    EUNISd_4 = col_character(),
    EUNISa_1_descr = col_character(),
    EUNISb_1_descr = col_character(),
    EUNISc_1_descr = col_character(),
    EUNISd_1_descr = col_character(),
    EUNIS_assignation = col_character(),
    EUNISa_2_descr = col_character(),
    EUNISa_3_descr = col_character(),
    EUNISa_4_descr = col_character(),
    EUNISb_2_descr = col_character(),
    EUNISb_3_descr = col_character(),
    EUNISb_4_descr = col_character(),
    EUNISc_2_descr = col_character(),
    EUNISc_3_descr = col_character(),
    EUNISc_4_descr = col_character(),
    EUNISd_2_descr = col_character(),
    EUNISd_3_descr = col_character(),
    EUNISd_4_descr = col_character()
    )
  )
```

No parsing issues!

# Merge RS data to the ReSurvey database

For some points, there is data both from S2 and Landsat. In those cases, use the S2 data because it is more precise (10 m vs 30 m).

```{r}
data_RS_S2_ID <- data_RS_S2_ID %>%
  rename_with(~ paste0(., "_S2"), starts_with("NDVI")) %>%
  rename_with(~ paste0(., "_S2"), starts_with("NDMI")) %>%
  rename_with(~ paste0(., "_S2"), starts_with("NDWI")) %>%
  rename_with(~ paste0(., "_S2"), starts_with("EVI")) %>%
  rename_with(~ paste0(., "_S2"), starts_with("SAVI")) %>%
  select(-source)
data_RS_Landsat_ID <- data_RS_Landsat_ID %>%
  rename_with(~ paste0(., "_Landsat"), starts_with("NDVI")) %>%
  rename_with(~ paste0(., "_Landsat"), starts_with("NDMI")) %>%
  rename_with(~ paste0(., "_Landsat"), starts_with("NDWI")) %>%
  rename_with(~ paste0(., "_Landsat"), starts_with("EVI")) %>%
  rename_with(~ paste0(., "_Landsat"), starts_with("SAVI")) %>%
  select(-source)
```

Join S2, S2_phen and Landsat data:

```{r}
data_RS <- data_RS_S2_ID %>% 
  full_join(data_RS_S2_phen_ID) %>%
  full_join(data_RS_Landsat_ID)
```

Number of observations with NDVI_max data from both S2 and Landsat:

```{r}
nrow(data_RS %>% filter(!is.na(NDVI_max_S2) & !is.na(NDVI_max_Landsat)))
```

Difference between NDVI_max values from S2 and Landsat:

```{r}
data_RS %>% filter(!is.na(NDVI_max_S2) & !is.na(NDVI_max_Landsat)) %>%
  mutate(diff_NDVI_max = abs(NDVI_max_S2 - NDVI_max_Landsat)) %>%
  ggplot(aes(x = diff_NDVI_max, fill = paste(biogeo, unit, sep = "-"))) +
  geom_histogram(color = "black") + 
  facet_wrap(~ paste(biogeo, unit, sep = "-")) + theme(legend.position = "none")
data_RS %>% filter(!is.na(NDMI_max_S2) & !is.na(NDMI_max_Landsat)) %>%
  mutate(diff_NDMI_max = abs(NDMI_max_S2 - NDMI_max_Landsat)) %>%
  ggplot(aes(x = diff_NDMI_max, fill = paste(biogeo, unit, sep = "-"))) +
  geom_histogram(color = "black") + 
  facet_wrap(~ paste(biogeo, unit, sep = "-")) + theme(legend.position = "none")
```

There is a large difference between NDVI values from S2 and Landsat. So far, use the S2 data, but checking with Bea / Jose.

When values are available from both satellites, use S2:

```{r}
data_RS <- data_RS %>%
  mutate(across(
    matches("^(NDVI|NDMI|NDWI|EVI|SAVI)_(max|median|min|mode|p10|p90)_S2$"),
    ~ case_when(
      # If both the current column and the corresponding Landsat column are NA,
      # set to NA_real_
      is.na(.x) & is.na(get(sub("_S2$", "_Landsat", cur_column()))) ~ NA_real_,
      # If the corresponding Landsat column is NA, use the current column's value
      is.na(get(sub("_S2$", "_Landsat", cur_column()))) ~ .x,
      # If the current column is NA, use the corresponding Landsat column's value
      is.na(.x) ~ get(sub("_S2$", "_Landsat", cur_column())),
      # Otherwise, use the current column's value
      TRUE ~ .x
      ), .names = "{col}_combined")) %>%
  rename_with(~ sub("_S2_combined$", "", .), matches("_S2_combined$"))
```

```{r}
db_resurv_RS <- db_resurv %>%
  left_join(data_RS %>% select(-obs_unique_id)) %>%
  left_join(data_RS_CH_ID %>% select(-obs_unique_id)) %>%
  mutate(S2_data = !is.na(NDVI_max_S2) & !is.na(NDMI_max_S2), 
         Landsat_data = !is.na(NDVI_max_Landsat) & !is.na(NDMI_max_Landsat),
         CH_data = !is.na(canopy_height),
         S2_phen_data = !is.na(SOS_DOY)) %>%
  # So far, remove cols for _S2 and _Landsat
  select(-matches("_(S2|Landsat)$"))
```

```{r}
db_resurv_RS %>% count(S2_data)
db_resurv_RS %>% count(Landsat_data)
db_resurv_RS %>% count(CH_data)
db_resurv_RS %>% count(S2_phen_data)
```

# Save to clean data

Save clean file for analyses (to be updated continuously due to updates in ReSurvey database and updates on RS data).

```{r}
write_tsv(db_resurv_RS,here("data", "clean","db_resurv_RS_20250505.csv"))
```

# Session info

```{r}
sessionInfo()
```
