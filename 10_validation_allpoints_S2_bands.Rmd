---
title: "Script to validate points in ReSurvey database using RS data (S2 only)"
subtitle: "Validation done with ALL points (all observations)"
author: "Alicia Vald√©s"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document: default
  html_notebook: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
```

This R script is used to validate the points in the ReSurvey database using RS indicators (indices + phenology + canopy height).

# Load libraries

```{r}
library(tidyverse)
library(here)
library(gridExtra)
library(readxl)
library(scales)
library(sf)
library(rnaturalearth)
library(dtplyr)
library(lme4)
library(lmerTest)
library(car)
library(ggeffects)
library(party)
library(partykit)
library(moreparty)
library(doParallel)
library(strucchange)
library(ggparty)
library(caret)
library(moreparty)
library(randomForest)
library(pROC)
library(corrplot)
library(rlang)
library(stringr)
library(beepr)
library(foreach)
library(permimp)
```

# Define printall function

```{r}
printall <- function(tibble) {
  print(tibble, width = Inf)
  }
```

# Load geom_flat_violin plot

```{r}
source("https://gist.githubusercontent.com/benmarwick/2a1bb0133ff568cbe28d/raw/fb53bd97121f7f9ce947837ef1a4c65a73bffb3f/geom_flat_violin.R")
```

# Load previously created objects

```{r}
# Define the folder path
folder_path <- here("objects", "10")

# List all .RData or .rda files in the folder
rdata_files <- list.files(folder_path, full.names = TRUE)

# Load each file
lapply(rdata_files, load, envir = .GlobalEnv)
```

# Read data

```{r}
data_validation<-read_tsv(here("data", "clean","final_RS_data_bands_S2_all.csv"))
```

No parsing issues!

# Some data managenemt

## TO-DO: Missing data checks

Do when all RS data is ready!

# Distributions all bioregions

## Indices

```{r}
# Define a function to create histograms
plot_histogram <- function(data, x_var, x_label) {
  ggplot(data %>%
           dplyr::filter(EUNISa_1 %in% c("T", "R", "S", "Q")),
         aes(x = !!sym(x_var))) +
    geom_histogram(color = "black", fill = "white") +
    labs(x = x_label, y = "Frequency") +
    theme_bw()
}
```

```{r}
# Define a function to create plots with violin + boxplot + points
distr_plot <- function(data, y_vars, y_labels) {
  for (i in seq_along(y_vars)) {
    y_var <- y_vars[[i]]
    y_label <- y_labels[[i]]
    
    p <- ggplot(data = data %>%
                  dplyr::filter(EUNISa_1 %in% c("T", "R", "S", "Q")),
                aes(x = EUNISa_1_descr, y = !!sym(y_var), fill = EUNISa_1_descr)) +
      geom_flat_violin(position = position_nudge(x = 0.2, y = 0), alpha = 0.8) +
      geom_point(aes(y = !!sym(y_var), color = EUNISa_1_descr),
                 position = position_jitter(width = 0.15), size = 1, alpha = 0.25) +
      geom_boxplot(width = 0.2, outlier.shape = NA, alpha = 0.5) +
      stat_summary(fun.y = mean, geom = "point", shape = 20, size = 1) +
      stat_summary(fun.data = function(x) data.frame(y = max(x) + 0.1,
                                                     label = length(x)),
                   geom = "text", aes(label = ..label..), vjust = 0.5) +
      labs(y = y_label, x = "EUNIS level 1") +
      scale_x_discrete(labels = function(x) str_wrap(x, width = 15)) +
      guides(fill = FALSE, color = FALSE) +
      theme_bw() + coord_flip()
    
    print(p)
  }
}
```

Ranges of min and max:

```{r}
range(data_validation$NDVI_max, na.rm = T) # NDVI_max > 1 (slightly)
range(data_validation$NDMI_max, na.rm = T) # NDMI_max > 1 (slightly)
range(data_validation$NDWI_max, na.rm = T)
range(data_validation$SAVI_max, na.rm = T)
range(data_validation$EVI_max, na.rm = T) # EVI_max > 1 (slightly)
range(data_validation$NDVI_min, na.rm = T)
range(data_validation$NDMI_min, na.rm = T)
range(data_validation$NDWI_min, na.rm = T) # NDWI_min < -1 (slightly)
range(data_validation$SAVI_min, na.rm = T)
range(data_validation$EVI_min, na.rm = T) # EVI_min < -1!
```

```{r}
nrow(data_validation %>% dplyr::filter(NDVI_max > 1))
nrow(data_validation %>% dplyr::filter(NDMI_max > 1))
nrow(data_validation %>% dplyr::filter(EVI_max > 1))
nrow(data_validation %>% dplyr::filter(NDWI_min < -1))
nrow(data_validation %>% dplyr::filter(EVI_min < -1))
```

Histograms to check that max and min values are ok:

```{r}
plot_histogram(data_validation, "NDVI_max", "NDVI max")
plot_histogram(data_validation, "NDMI_max", "NDMI max")
plot_histogram(data_validation, "NDWI_max", "NDWI max")
plot_histogram(data_validation, "SAVI_max", "SAVI max")
plot_histogram(data_validation, "EVI_max", "EVI max")
plot_histogram(data_validation, "NDVI_min", "NDVI min")
plot_histogram(data_validation, "NDMI_min", "NDMI min")
plot_histogram(data_validation, "NDWI_min", "NDWI min")
plot_histogram(data_validation, "SAVI_min", "SAVI min")
plot_histogram(data_validation, "EVI_min", "EVI min")
```

```{r}
nrow(data_validation %>%
       dplyr::filter(EUNISa_1 %in% c("T", "R", "S", "Q")) %>%
       dplyr::filter(EVI_max > 1 | EVI_max < -1))
data_validation %>%
       dplyr::filter(EUNISa_1 %in% c("T", "R", "S", "Q"))%>%
  dplyr::filter(EVI_max > 1 | EVI_max < -1) %>%
  count(biogeo, unit)
```

Most EVI values are ok!

Distribution plots:

```{r message=FALSE, warning=FALSE}
distr_plot(data_validation %>% dplyr::filter(EVI_min > -0.5),
           c("NDVI_max", "EVI_max", "SAVI_max", "NDMI_max", "NDWI_max",
             "NDVI_min", "EVI_min", "SAVI_min", "NDMI_min", "NDWI_min"),
           c("NDVI_max", "EVI_max", "SAVI_max", "NDMI_max", "NDWI_max",
             "NDVI_min", "EVI_min", "SAVI_min", "NDMI_min", "NDWI_min"))
```

## CH

```{r}
distr_plot(data_validation, "canopy_height", "Canopy height (m)")
```
 
### Show habitats with CH categories

```{r}
ggplot(data_validation %>%
         # Keep only forests, grasslands, shrublands and wetlands
         dplyr::filter(EUNISa_1 %in% c("T", "R", "S", "Q")) %>%
         mutate(CH_cat =
                  factor(
                    case_when(canopy_height == 0 ~ "0 m",
                              canopy_height > 0 & canopy_height <= 1 ~ "0-1 m",
                              canopy_height > 1 & canopy_height <=2 ~ "1-2 m",
                              canopy_height > 2 & canopy_height <=5 ~ "2-5 m",
                              canopy_height > 5 & canopy_height <=8 ~ "5-8 m",
                              canopy_height > 8 ~ "> 8 m",
                              is.na(canopy_height) ~ NA_character_),
                    levels = c(
                      "0 m", "0-1 m", "1-2 m", "2-5 m", "5-8 m", "> 8 m"))),
       aes(x = EUNISa_1_descr, fill = CH_cat)) +
  geom_bar() + theme_bw() + coord_flip() +
  scale_y_continuous(labels = label_number()) +
  scale_fill_viridis_d(direction = -1) +
  labs(x = "EUNIS level 1", fill = "Canopy height") +
  scale_x_discrete(labels = function(x) str_wrap(x, width = 15)) +
  theme(legend.position = c(0.8, 0.75),
        legend.direction = "vertical")
```

### Stats per habitat type

```{r}
data_validation %>%
  # Keep only forests, grasslands, shrublands and wetlands
  dplyr::filter(EUNISa_1 %in% c("T", "R", "S", "Q")) %>%
  group_by(EUNISa_1_descr) %>%
  summarise(across(canopy_height, list(
    mean = mean,
    median = median,
    sd = sd,
    min = min,
    max = max
    ), na.rm = TRUE))
```

## Phenology

Only using NDVI- and SAVI-based values so far.

Maximum NDVI should be equal to value at peak?

```{r}
nrow(data_validation %>% dplyr::filter(NDVI_pos_value  != NDVI_max))
```

Not sure why this happens, but in most cases the difference is small (< -0.1). Anyway, we should use only one of these two in the RF models.

```{r}
plot_histogram(data_validation, "NDVI_sos_doy", "NDVI_sos_doy")
plot_histogram(data_validation, "NDVI_pos_doy", "NDVI_pos_doy")
plot_histogram(data_validation, "NDVI_eos_doy", "NDVI_eos_doy")
plot_histogram(data_validation, "EVI_sos_doy", "EVI_sos_doy")
plot_histogram(data_validation, "EVI_pos_doy", "EVI_pos_doy")
plot_histogram(data_validation, "EVI_eos_doy", "EVI_eos_doy")
plot_histogram(data_validation, "SAVI_sos_doy", "SAVI_sos_doy")
plot_histogram(data_validation, "SAVI_pos_doy", "SAVI_pos_doy")
plot_histogram(data_validation, "SAVI_eos_doy", "SAVI_eos_doy")
```

```{r}
plot_histogram(data_validation, "NDVI_sos_value", "NDVI_sos_value")
plot_histogram(data_validation, "NDVI_pos_value", "NDVI_pos_value")
plot_histogram(data_validation, "NDVI_eos_value", "NDVI_eos_value")
plot_histogram(data_validation, "EVI_sos_value", "EVI_sos_value")
plot_histogram(data_validation, "EVI_pos_value", "EVI_pos_value")
plot_histogram(data_validation, "EVI_eos_value", "EVI_eos_value")
```

```{r}
plot_histogram(data_validation, "NDVI_gsd", "NDVI_gsd")
plot_histogram(data_validation, "EVI_gsd", "EVI_gsd")
plot_histogram(data_validation, "SAVI_gsd", "SAVI_gsd")
plot_histogram(data_validation, "NDVI_diff_pos_sos_value",
               "NDVI_diff_pos_sos_value")
plot_histogram(data_validation, "EVI_diff_pos_sos_value",
               "EVI_diff_pos_sos_value")
plot_histogram(data_validation, "SAVI_diff_pos_sos_value",
               "SAVI_diff_pos_sos_value")
plot_histogram(data_validation, "NDVI_diff_pos_eos_value",
               "NDVI_diff_pos_eos_value")
plot_histogram(data_validation, "EVI_diff_pos_eos_value",
               "EVI_diff_pos_eos_value")
plot_histogram(data_validation, "SAVI_diff_pos_eos_value",
               "SAVI_diff_pos_eos_value")
plot_histogram(data_validation, "NDVI_diff_pos_sos_doy",
               "NDVI_diff_pos_sos_doy")
plot_histogram(data_validation, "EVI_diff_pos_sos_doy",
               "EVI_diff_pos_sos_doy")
plot_histogram(data_validation, "SAVI_diff_pos_sos_doy",
               "SAVI_diff_pos_sos_doy")
plot_histogram(data_validation, "NDVI_diff_eos_pos_doy",
               "NDVI_diff_eos_pos_doy")
plot_histogram(data_validation, "EVI_diff_eos_pos_doy", 
               "EVI_diff_eos_pos_doy")
plot_histogram(data_validation, "SAVI_diff_eos_pos_doy", 
               "SAVI_diff_eos_pos_doy")
```

```{r}
plot_histogram(data_validation, "NDVI_auc", "NDVI_auc")
plot_histogram(data_validation, "EVI_auc", "EVI_auc")
plot_histogram(data_validation, "SAVI_auc", "SAVI_auc")
```

```{r}
distr_plot(data_validation,
           c("NDVI_sos_value","NDVI_pos_value", "NDVI_eos_value",
             "EVI_sos_value","EVI_pos_value", "EVI_eos_value",
             "SAVI_sos_value", "SAVI_pos_value", "SAVI_eos_value",
             "NDVI_sos_doy","NDVI_pos_doy", "NDVI_eos_doy",
             "EVI_sos_doy","EVI_pos_doy", "EVI_eos_doy",
             "SAVI_sos_doy", "SAVI_pos_doy", "SAVI_eos_doy"),
           c("NDVI_sos_value","NDVI_pos_value", "NDVI_eos_value",
             "EVI_sos_value","EVI_pos_value", "EVI_eos_value",
             "SAVI_sos_Value", "SAVI_pos_value", "SAVI_eos_value",
             "NDVI_sos_doy","NDVI_pos_doy", "NDVI_eos_doy",
             "EVI_sos_doy","EVI_pos_doy", "EVI_eos_doy",
             "SAVI_sos_doy", "SAVI_pos_doy", "SAVI_eos_doy")
           )
```

```{r}
distr_plot(data_validation,
           c("NDVI_gsd","EVI_gsd", "SAVI_gsd",
             "NDVI_diff_pos_sos_value", "EVI_diff_pos_sos_value",
             "SAVI_diff_pos_sos_value",
             "NDVI_diff_pos_eos_value", "EVI_diff_pos_eos_value",
             "SAVI_diff_pos_eos_value",
             "NDVI_diff_pos_sos_doy", "EVI_diff_pos_sos_doy",
             "SAVI_diff_pos_sos_doy",
             "NDVI_diff_eos_pos_doy", "EVI_diff_eos_pos_doy",
             "SAVI_diff_eos_pos_doy"),
           c("NDVI_gsd","EVI_gsd", "SAVI_gsd",
             "NDVI_diff_pos_sos_value", "EVI_diff_pos_sos_value",
             "SAVI_diff_pos_sos_Value",
             "NDVI_diff_pos_eos_value", "EVI_diff_pos_eos_value",
             "SAVI_diff_pos_eos_value",
             "NDVI_diff_pos_sos_doy", "EVI_diff_pos_sos_doy",
             "SAVI_diff_pos_sos_doy",
             "NDVI_diff_eos_pos_doy", "EVI_diff_eos_pos_doy",
             "SAVI_diff_eos_pos_doy")
           )
```

```{r}
distr_plot(data_validation,
           c("NDVI_auc", "EVI_auc", "SAVI_auc"),
             c("NDVI_auc", "EVI_auc", "SAVI_auc"))
```

# TBD: Distributions per bioregion

```{r}
# Define a function to create plots with violin + boxplot + points
distr_plot_biogeo <- function(data, y_vars, y_labels) {
  plots <- list()
  
  for (i in seq_along(y_vars)) {
    y_var <- y_vars[[i]]
    y_label <- y_labels[[i]]
    
    p <- ggplot(data = data %>%
                  dplyr::filter(EUNISa_1 %in% c("T", "R", "S", "Q")),
                aes(x = EUNISa_1_descr, y = !!sym(y_var), fill = EUNISa_1_descr)) +
      geom_flat_violin(position = position_nudge(x = 0.2, y = 0), alpha = 0.8) +
      geom_point(aes(y = !!sym(y_var), color = EUNISa_1_descr),
                 position = position_jitter(width = 0.15), size = 1, alpha = 0.25) +
      geom_boxplot(width = 0.2, outlier.shape = NA, alpha = 0.5) +
      stat_summary(fun.y = mean, geom = "point", shape = 20, size = 1) +
      stat_summary(fun.data = function(x) data.frame(y = max(x) + 0.1,
                                                     label = length(x)),
                   geom = "text", aes(label = ..label..), vjust = 0.5) +
      labs(y = y_label, x = "EUNISa_1_descr") +
      scale_x_discrete(labels = function(x) str_wrap(x, width = 15)) +
      guides(fill = FALSE, color = FALSE) +
      theme_bw() + coord_flip() + facet_wrap(~ biogeo)
    
    plots[[y_var]] <- p
  }
  
  return(plots)
}
```

## Indices

Distribution plots:

## CH

```{r}
distr_plot_biogeo(data_validation, "canopy_height", "Canopy height (m)")
```

## Phenology

# Define functions for RF models

## Function for fitting RF models

RF models fitted using the conditional inference version of random forest (first cforest in package party, now fastcforest in package moreparty). Suggested if the data are highly correlated. Cforest is more stable in deriving variable importance values in the presence of highly correlated variables, thus providing better accuracy in calculating variable importance (ref below).

Hothorn, T., Hornik, K. and Zeileis, A. (2006) Unbiased Recursive Portioning: A Conditional Inference Framework. Journal of Computational and Graphical Statistics, 15, 651-
674. http://dx.doi.org/10.1198/106186006X133933

Choose the hyperparameter mtry based on the square root of the number of predictor variables:

Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical
learning: Data mining, inference, and prediction. Springer Science &
Business Media.

Maybe TO_DO:
We variated ntree from 50 to 800 in steps of 50, leaving mtry constant at 2. Tis parameter variation showed that ntree=500 was optimal, while higher ntree led to no further model improvement (Supplementary Fig. S10). Subsequently, the hyperparameter mtry was varied from 2 to 8 with constant ntree=500. Here, mtry=3 led to the best results in almost all cases (Supplementary Fig. S11). Consequently, we chose ntree=500 and mtry=3 for our main analysis across all study sites.

Define a function to run fastcforest models:

```{r}
run_rf <- function(vars_RF, train_data, response_var, ntree = 500) 
  {
  
  # Detect and register available cores (leave one free)
  n_cores <- parallel::detectCores() - 1
  cl <- makeCluster(n_cores)
  registerDoParallel(cl)
  
  train_name <- deparse(substitute(train_data))
  
  # Export necessary variables to the cluster
  clusterExport(cl, varlist = c("vars_RF", train_name))
  
  # Set seed for reproducibility
  set.seed(123)
  
  # Measure execution time
  execution_time <- system.time({
    rf_model <- fastcforest(
      formula = reformulate(vars_RF, response = response_var),
      data = train_data,
      controls = party::cforest_control(
        mtry = round(sqrt(length(vars_RF))),
        ntree = ntree
      ),
      parallel = TRUE
    )
  })
  
  # Stop the cluster
  stopCluster(cl)
  
  # Return both the model and execution time
  list(model = rf_model, time = execution_time)
}
```

## Function to compute variable importance

```{r}
# compute_varimp <- function(model, nperm = 100, 
#                                    n_cores = parallel::detectCores() - 1) {
#   # Set up parallel backend
#   cl <- makeCluster(n_cores)
#   registerDoParallel(cl)
#   
#   # Measure execution time
#   execution_time <- system.time({
#     varimp_list <- foreach(i = 1:nperm, .combine = '+', 
#                            .packages = "party") %dopar% {
#       varimp(model, conditional = FALSE, nperm = 1)
#     }
#   })
#   
#   stopCluster(cl)
#   
#   # Average the results
#   varimp_avg <- varimp_list / nperm
#   
#   return(list(varimp = varimp_avg, time = execution_time))
# }
```

Using permimp() en permimp package:
https://cran.r-project.org/web/packages/permimp/vignettes/permimp-package.html#fn1

```{r}
compute_varimp <- function(model, nperm = 100) {

  # Measure execution time
  execution_time <- system.time({
    varimp_result <- permimp(model, conditional = FALSE, progressBar = TRUE)
  })

  return(list(varimp = varimp_result, time = execution_time))
}
```

## Function to compute CONDITIONAL variable importance

```{r}
compute_varimp_cond <- function(model, nperm = 100) {

  # Measure execution time
  execution_time <- system.time({
    varimp_result <- permimp(model, conditional = TRUE, progressBar = TRUE)
  })

  return(list(varimp = varimp_result, time = execution_time))
}
```

## Function to compute ROC (level 1)

```{r}
compute_roc_level1 <- function(model, test_data) {
  # Measure execution time
  execution_time <- system.time({
    # Step 1: Predict probabilities
    probabilities <- predict(model, newdata = test_data, type = "prob")
    
    # Step 2: Convert list of matrices to a proper data frame
    prob_matrix <- t(sapply(probabilities, as.vector))
    prob_df <- as.data.frame(prob_matrix)
    colnames(prob_df) <- c("Q", "R", "S", "T")
    
    # Step 3: Prepare actual class labels
    actual <- factor(test_data$EUNISa_1, levels = c("Q", "R", "S", "T"))
    
    # Step 4: Binarize actual labels
    actual_bin <- model.matrix(~ actual - 1)
    colnames(actual_bin) <- gsub("actual", "", colnames(actual_bin))
    
    # Step 5: Compute ROC data for each class
    roc_data <- lapply(levels(actual), function(class) {
      roc_obj <- roc(actual_bin[, class], prob_df[[class]])
      auc_val <- round(auc(roc_obj), 3)
      data.frame(
        FPR = rev(roc_obj$specificities),
        TPR = rev(roc_obj$sensitivities),
        Class = paste0(class, " (AUC = ", auc_val, ")")
      )
    }) %>% bind_rows()
  })
  
  # Return both ROC data and execution time
  return(list(roc = roc_data, time = execution_time))
}
```

## Function to compute ROC (level 2)

```{r}
compute_roc_level2 <- function(model, test_data) {
  # Measure execution time
  execution_time <- system.time({
    # Step 1: Predict probabilities
    probabilities <- predict(model, newdata = test_data, type = "prob")
    
    # Step 2: Convert list of matrices to a proper data frame
    prob_matrix <- t(sapply(probabilities, as.vector))
    prob_df <- as.data.frame(prob_matrix)
    colnames(prob_df) <- c("Q1", "Q2", "Q4", "Q5", "R1", "R2", "R3", "R4", "R5",
                           "R6", "S3", "S4", "T1", "T3")
    
    # Step 3: Prepare actual class labels
    actual <- factor(test_data$EUNISa_2, 
                     levels = c("Q1", "Q2", "Q4", "Q5", "R1", "R2", "R3", "R4",
                                "R5", "R6", "S3", "S4", "T1", "T3"))
    
    # Step 4: Binarize actual labels
    actual_bin <- model.matrix(~ actual - 1)
    colnames(actual_bin) <- gsub("actual", "", colnames(actual_bin))
    
    # Step 5: Compute ROC data for each class
    roc_data <- lapply(levels(actual), function(class) {
      roc_obj <- roc(actual_bin[, class], prob_df[[class]])
      auc_val <- round(auc(roc_obj), 3)
      data.frame(
        FPR = rev(roc_obj$specificities),
        TPR = rev(roc_obj$sensitivities),
        Class = paste0(class, " (AUC = ", auc_val, ")")
      )
    }) %>% bind_rows()
  })
  
  # Return both ROC data and execution time
  return(list(roc = roc_data, time = execution_time))
}
```

# Define list of predictor vars

```{r}
vars_RF <- c(
  # Min values of all indices
  "NDVI_min", "EVI_min", "NDMI_min", "NDWI_min", "SAVI_min",
  # Max values of NDMI and NDWI
  "NDMI_max", "NDWI_max",
  # AUC of NDVI, EVI and SAVI
  "NDVI_auc", "EVI_auc", "SAVI_auc",
  # Values of NDVI, EVI and SAVI at sos, pos and eos
  "NDVI_sos_value", "NDVI_pos_value", "NDVI_eos_value",
  "EVI_sos_value", "EVI_pos_value", "EVI_eos_value", 
  "SAVI_sos_value", "SAVI_pos_value", "SAVI_eos_value",
  # Differences pos-sos in value and doy
  "NDVI_diff_pos_sos_value", "EVI_diff_pos_sos_value", "SAVI_diff_pos_sos_value",
  "NDVI_diff_pos_sos_doy", "EVI_diff_pos_sos_doy", "SAVI_diff_pos_sos_doy",
  # Differences pos-eos in value and doy
  "NDVI_diff_pos_eos_value", "EVI_diff_pos_eos_value","SAVI_diff_pos_eos_value",
  "NDVI_diff_eos_pos_doy", "EVI_diff_eos_pos_doy", "SAVI_diff_eos_pos_doy",
  # Growing season duration
  "NDVI_gsd", "EVI_gsd", "SAVI_gsd",
  # Canopy height
  "canopy_height")
```

# Correlation

```{r}
filtered_data0 <- data_validation %>%
  mutate(EUNISa_1 = as.factor(EUNISa_1)) %>%
  # Remove all rows with wrong values of indices (not between -1 and 1)
  dplyr::filter(EVI_max <= 1 & EVI_min >= -1) %>%
  dplyr::filter(NDVI_max <= 1) %>%
  dplyr::filter(NDMI_max <= 1) %>%
  dplyr::filter(NDWI_min >= -1) %>%
  # Remove rows with missing values in predictors
  dplyr::filter(if_all(all_of(vars_RF), ~ !is.na(.))) %>%
  # Keep only rows with differences > 0
  dplyr::filter(if_all(contains("diff"), ~ .x > 0)) %>%
  # Select only variables needed
  select(PlotObservationID, EUNISa_1, all_of(vars_RF))
```

Correlation of all variables to be included in RF models:

```{r}
corrplot(filtered_data0 %>% 
           dplyr::select(all_of(vars_RF)) %>%
           cor(use = "pairwise.complete.obs"),
         method = "color", type = "upper", tl.col = "black", tl.srt = 45)
```

```{r}
# Compute correlation matrix
cor_matrix <- filtered_data0 %>%
  dplyr::select(all_of(vars_RF)) %>%
  cor(use = "pairwise.complete.obs")

# Get the upper triangle of the matrix without the diagonal
cor_matrix[lower.tri(cor_matrix, diag = TRUE)] <- NA

# Find variable pairs with correlation > 0.7 or < -0.7
high_corr_pairs <- which(abs(cor_matrix) > 0.7, arr.ind = TRUE)

# Create a data frame of the results
cor_pairs_df <- data.frame(
  Var1 = rownames(cor_matrix)[high_corr_pairs[, 1]],
  Var2 = colnames(cor_matrix)[high_corr_pairs[, 2]],
  Correlation = cor_matrix[high_corr_pairs]
)

# View the result
print(cor_pairs_df)
```


# Define list of uncorrelated predictor vars

```{r}
vars_RF_uncorr <- c(
  # Min values
  "NDMI_min", "NDWI_min", "SAVI_min",
  # Max value of NDMI
  "NDMI_max",
  # AUC
  "SAVI_auc",
  # Values of SAVI at sos, pos and eos
  "SAVI_sos_value", "SAVI_pos_value", "SAVI_eos_value",
  # Differences pos-sos in value and doy
  "SAVI_diff_pos_sos_value", "SAVI_diff_pos_sos_doy",
  # Differences pos-eos in value and doy
  "SAVI_diff_pos_eos_value", "SAVI_diff_eos_pos_doy",
  # Canopy height
  "canopy_height")
```

# RF models with no previous validation

## 0: RF with all points

Split filtered_data0 into training and test data sets.

```{r}
set.seed(123) 
train_indices0 <- sample(1:nrow(filtered_data0), 0.7 * nrow(filtered_data0))
train_data0 <- filtered_data0[train_indices0, ]
test_data0 <- filtered_data0[-train_indices0, ]
```

Number of points per category for filtered data:

```{r}
filtered_data0 %>% count(EUNISa_1)
```

```{r eval=FALSE, include=FALSE}
rf0_S2 <- run_rf(vars_RF, train_data0)
print(rf0_S2$time)
save(rf0_S2, file = "objects/10/rf0_S2.Rdata")
```

```{r eval=FALSE, include=FALSE}
predictions_rf0_S2 <- predict(rf0_S2$model, newdata = test_data0,OOB = TRUE,
                           type = "response")
save(predictions_rf0_S2, file = "objects/10/predictions_rf0_S2.Rdata")
```

Confusion matrix:

```{r}
confusionMatrix(predictions_rf0_S2, test_data0$EUNISa_1)
```

```{r eval=FALSE, include=FALSE}
varimp_rf0_S2 <- compute_varimp(rf0_S2$model, nperm = 100)
print(varimp_rf0_S2$time)
save(varimp_rf0_S2, file = "objects/10/varimp_rf0_S2.Rdata")
```

```{r eval=FALSE, include=FALSE}
varimp_rf0_cond_S2 <- compute_varimp_cond(rf0_S2$model, nperm = 100)
print(varimp_rf0_cond_S2$time)
save(varimp_rf0_cond_S2, file = "objects/10/varimp_rf0_cond_S2.Rdata")
```

Variable Importance Plot

```{r}
plot(varimp_rf0_S2$varimp, margin = c(9, 3, 2, 2))
plot(varimp_rf0_cond_S2$varimp, margin = c(9, 3, 2, 2))
```

ROC curves:

```{r eval=FALSE, include=FALSE}
roc_data0_S2 <- compute_roc_level1(rf0_S2$model, test_data0)
print(roc_data0_S2$time)
save(roc_data0_S2, file = "objects/10/roc_data0_S2.Rdata")
```

```{r}
roc0 <- ggplot(roc_data0_S2$roc, aes(x = FPR, y = TPR, color = Class)) +
  geom_line(size = 1.2) +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(
    title = "Multiclass ROC Curves with AUC",
    x = "False Positive Rate",
    y = "True Positive Rate",
    color = "Class (AUC)"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
roc0
```

## 1: RF with all GPS points (diff or not)

```{r}
filtered_data1 <- data_validation %>%
  # Select only GPS points
  dplyr::filter(Lctnmth == "Location with GPS" |
                  Lctnmth == "Location with differential GPS") %>%
  mutate(EUNISa_1 = as.factor(EUNISa_1)) %>%
  # Remove all rows with wrong values of indices (not between -1 and 1)
  dplyr::filter(EVI_max <= 1 & EVI_min >= -1) %>%
  dplyr::filter(NDVI_max <= 1) %>%
  dplyr::filter(NDMI_max <= 1) %>%
  dplyr::filter(NDWI_min >= -1) %>%
  # Remove rows with missing values in predictors
  dplyr::filter(if_all(all_of(vars_RF), ~ !is.na(.))) %>%
  # Keep only rows with differences > 0
  dplyr::filter(if_all(contains("diff"), ~ .x > 0)) %>%
  # Select only variables needed
  select(PlotObservationID, EUNISa_1, all_of(vars_RF))
```

Split into training and test data sets.

```{r}
set.seed(123)
train_indices1 <- sample(1:nrow(filtered_data1), 0.7 * nrow(filtered_data1))
train_data1 <- filtered_data1[train_indices1, ]
test_data1 <- filtered_data1[-train_indices1, ]
```

Number of points per category for filtered data:

```{r}
filtered_data1 %>% count(EUNISa_1)
```

```{r eval=FALSE, include=FALSE}
rf1_S2 <- run_rf(vars_RF, train_data1, "EUNISa_1")
print(rf1_S2$time)
save(rf1_S2, file = "objects/10/rf1_S2.Rdata")
```

```{r eval=FALSE, include=FALSE}
predictions_rf1_S2 <- predict(rf1_S2$model, newdata = test_data1,OOB = TRUE,
                           type = "response")
save(predictions_rf1_S2, file = "objects/10/predictions_rf1_S2.Rdata")
```

Confusion matrix:

```{r}
confusionMatrix(predictions_rf1_S2, test_data1$EUNISa_1)
```

```{r eval=FALSE, include=FALSE}
varimp_rf1_S2 <- compute_varimp(rf1_S2$model, nperm = 100)
print(varimp_rf1_S2$time)
save(varimp_rf1_S2, file = "objects/10/varimp_rf1_S2.Rdata")
```

```{r eval=FALSE, include=FALSE}
varimp_rf1_cond_S2 <- compute_varimp_cond(rf1_S2$model, nperm = 100)
print(varimp_rf1_cond_S2$time)
save(varimp_rf1_cond_S2, file = "objects/10/varimp_rf1_cond_S2.Rdata")
```

Variable Importance Plot

```{r}
plot(varimp_rf1_S2$varimp, margin = c(9, 3, 2, 2))
plot(varimp_rf1_cond_S2$varimp, margin = c(9, 3, 2, 2))
```

ROC curves:

```{r eval=FALSE, include=FALSE}
roc_data1_S2 <- compute_roc_level1(rf1_S2$model, test_data1)
print(roc_data1_S2$time)
save(roc_data1_S2, file = "objects/10/roc_data1_S2.Rdata")
```

```{r}
roc1 <- ggplot(roc_data1_S2$roc, aes(x = FPR, y = TPR, color = Class)) +
  geom_line(size = 1.2) +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(
    title = "Multiclass ROC Curves with AUC",
    x = "False Positive Rate",
    y = "True Positive Rate",
    color = "Class (AUC)"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
roc1
```

# Rough validation

Define a set of rules for a first validation of ALL ReSurvey data ("Expert-based" rules). Not very ambitious, only taking out observations that are clearly wrong on the basis of indicator values.

Create column for first validation based on different indicators, where "wrong" is noted when the validation rule is not met. 

Define rules:

```{r}
data_validation <-
  data_validation %>%
  mutate(
    valid_1_NDWI = case_when(
      # Points that are basically water
      NDWI_max > 0.3 ~ "wrong",
      TRUE ~ NA_character_),
    valid_1_CH = case_when(
      # R & Q points with high CH
      EUNISa_1 %in% c("R", "Q") & canopy_height > 2 ~ "wrong",
      # T points with low CH
      EUNISa_1 == "T" & canopy_height < 3 ~ "wrong",
      # S points with high CH
      EUNISa_1 == "S" & canopy_height > 3 ~ "wrong",
      TRUE ~ NA_character_),
    # No rules for NDVI, we will take out observations based on distributions
    # Count how many validation rules are not met
    valid_1_count = rowSums(across(c(valid_1_NDWI, valid_1_CH), ~ . == "wrong"),
                            na.rm = TRUE),
    # Points where at least 1 rule not met
    valid_1 = if_else(valid_1_count > 0, "At least 1 rule broken",
                      "No rules broken so far")
    )
```

### Plots rough validation

```{r}
ggplot(data_validation%>%
         mutate(rules_broken = case_when(
           valid_1_count == 1 & valid_1_NDWI == "wrong" ~ "NDWI",
           valid_1_count == 1 & valid_1_CH == "wrong" ~ "CH",
           valid_1_count == 2 &
             valid_1_NDWI == "wrong" & valid_1_CH == "wrong"~ "NDWI + CH",
           TRUE ~ NA_character_
         )), 
       aes(x = valid_1_count, fill = rules_broken)) +
  geom_bar() + labs(x = "Number of broken rules")
```

Proportion of observations not validated (so far):

```{r}
nrow(data_validation %>% dplyr::filter(valid_1_count > 0))/
  nrow(data_validation)
```

But be aware that there are still MANY missing RS data.

```{r}
ggplot(data_validation %>%
         mutate(diff_GPS = if_else(
           Lctnmth != "Location with differential GPS" |
             is.na(Lctnmth), "no", "yes")), 
       aes(x = diff_GPS, fill = valid_1)) +
  geom_bar() + labs(x = "Differential GPS")
ggplot(data_validation %>%
         mutate(GPS = case_when(
           Lctnmth == "Location with differential GPS" ~ "yes",
           Lctnmth == "Location with GPS" ~ "yes",
           is.na(Lctnmth) ~ "no",
           TRUE ~ "no"
         )), 
       aes(x = GPS, fill = valid_1)) +
  geom_bar() + labs(x = "GPS")
```

# RF models after rough validation

## 2: RF with all points after rough validation

```{r}
filtered_data2 <- data_validation %>%
  mutate(EUNISa_1 = as.factor(EUNISa_1)) %>%
  # Remove all rows with wrong values of indices (not between -1 and 1)
  dplyr::filter(EVI_max <= 1 & EVI_min >= -1) %>%
  dplyr::filter(NDVI_max <= 1) %>%
  dplyr::filter(NDMI_max <= 1) %>%
  dplyr::filter(NDWI_min >= -1) %>%
  # Remove rows with missing values in predictors
  dplyr::filter(if_all(all_of(vars_RF), ~ !is.na(.))) %>%
  # Keep only rows with differences > 0
  dplyr::filter(if_all(contains("diff"), ~ .x > 0)) %>%
  # Filter out points that have not passed the rough validation
  filter(valid_1 == "No rules broken so far") %>%
  # Select only variables needed
  select(PlotObservationID, EUNISa_1, all_of(vars_RF)) 
```

Split into training and test data sets.

```{r}
set.seed(123)
train_indices2 <- sample(1:nrow(filtered_data2), 0.7 * nrow(filtered_data2))
train_data2 <- filtered_data2[train_indices2, ]
test_data2 <- filtered_data2[-train_indices2, ]
```

Number of points per category for filtered data:

```{r}
filtered_data2 %>% count(EUNISa_1)
```

```{r eval=FALSE, include=FALSE}
rf2_S2 <- run_rf(vars_RF, train_data2, "EUNISa_1")
print(rf2_S2$time)
save(rf2_S2, file = "objects/10/rf2_S2.Rdata")
```

```{r eval=FALSE, include=FALSE}
predictions_rf2_S2 <- predict(rf2_S2$model, newdata = test_data2,OOB = TRUE,
                           type = "response")
save(predictions_rf2_S2, file = "objects/10/predictions_rf2_S2.Rdata")
```

Confusion matrix:

```{r}
confusionMatrix(predictions_rf2_S2, test_data2$EUNISa_1)
```

```{r eval=FALSE, include=FALSE}
varimp_rf2_S2 <- compute_varimp(rf2_S2$model, nperm = 100)
save(varimp_rf2_S2, file = "objects/10/varimp_rf2_S2.Rdata")
```

```{r eval=FALSE, include=FALSE}
varimp_rf2_cond_S2 <- compute_varimp_cond(rf2_S2$model, nperm = 100)
print(varimp_rf2_cond_S2$time)
save(varimp_rf2_cond_S2, file = "objects/10/varimp_rf2_cond_S2.Rdata")
```

Variable Importance Plot

```{r}
plot(varimp_rf2_S2$varimp, margin = c(9, 3, 2, 2))
plot(varimp_rf2_cond_S2$varimp, margin = c(9, 3, 2, 2))
```

ROC curves:

```{r eval=FALSE, include=FALSE}
roc_data2_S2 <- compute_roc_level1(rf2_S2$model, test_data2)
save(roc_data2_S2, file = "objects/10/roc_data2_S2.Rdata")
```

```{r}
roc2 <- ggplot(roc_data2_S2$roc, aes(x = FPR, y = TPR, color = Class)) +
  geom_line(size = 1.2) +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(
    title = "Multiclass ROC Curves with AUC",
    x = "False Positive Rate",
    y = "True Positive Rate",
    color = "Class (AUC)"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
roc2
```

## 3: RF with GPS points after rough validation

```{r}
filtered_data3 <- data_validation %>%
  # Select only GPS points
  dplyr::filter(Lctnmth == "Location with GPS" |
                  Lctnmth == "Location with differential GPS") %>%
  mutate(EUNISa_1 = as.factor(EUNISa_1)) %>%
  # Remove all rows with wrong values of indices (not between -1 and 1)
  dplyr::filter(EVI_max <= 1 & EVI_min >= -1) %>%
  dplyr::filter(NDVI_max <= 1) %>%
  dplyr::filter(NDMI_max <= 1) %>%
  dplyr::filter(NDWI_min >= -1) %>%
  # Remove rows with missing values in predictors
  dplyr::filter(if_all(all_of(vars_RF), ~ !is.na(.))) %>%
  # Keep only rows with differences > 0
  dplyr::filter(if_all(contains("diff"), ~ .x > 0)) %>%
  # Filter out points that have not passed the rough validation
  filter(valid_1 == "No rules broken so far") %>%
  # Select only variables needed
  select(PlotObservationID, EUNISa_1, all_of(vars_RF))
```

Split into training and test data sets.

```{r}
set.seed(123)
train_indices3 <- sample(1:nrow(filtered_data3), 0.7 * nrow(filtered_data3))
train_data3 <- filtered_data3[train_indices3, ]
test_data3 <- filtered_data3[-train_indices3, ]
```

Number of points per category for filtered data:

```{r}
filtered_data3 %>% count(EUNISa_1)
```

```{r eval=FALSE, include=FALSE}
rf3_S2 <- run_rf(vars_RF, train_data3, "EUNISa_1")
print(rf3_S2$time)
save(rf3_S2, file = "objects/10/rf3_S2.Rdata")
```

```{r eval=FALSE, include=FALSE}
predictions_rf3_S2 <- predict(rf3_S2$model, newdata = test_data3,OOB = TRUE,
                           type = "response")
save(predictions_rf3_S2, file = "objects/10/predictions_rf3_S2.Rdata")
```

Confusion matrix:

```{r}
confusionMatrix(predictions_rf3_S2, test_data3$EUNISa_1)
```

```{r eval=FALSE, include=FALSE}
varimp_rf3_S2 <- compute_varimp(rf3_S2$model, nperm = 100)
save(varimp_rf3_S2, file = "objects/10/varimp_rf3_S2.Rdata")
```

```{r eval=FALSE, include=FALSE}
varimp_rf3_cond_S2 <- compute_varimp_cond(rf3_S2$model, nperm = 100)
print(varimp_rf3_cond_S2$time)
save(varimp_rf3_cond_S2, file = "objects/10/varimp_rf3_cond_S2.Rdata")
```

Variable Importance Plot

```{r}
plot(varimp_rf3_S2$varimp, margin = c(9, 3, 2, 2))
plot(varimp_rf3_cond_S2$varimp, margin = c(9, 3, 2, 2))
```

ROC curves:

```{r eval=FALSE, include=FALSE}
roc_data3_S2 <- compute_roc_level1(rf3_S2$model, test_data3)
save(roc_data3_S2, file = "objects/10/roc_data3_S2.Rdata")
```

```{r}
roc3 <- ggplot(roc_data3_S2$roc, aes(x = FPR, y = TPR, color = Class)) +
  geom_line(size = 1.2) +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(
    title = "Multiclass ROC Curves with AUC",
    x = "False Positive Rate",
    y = "True Positive Rate",
    color = "Class (AUC)"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
roc3
```

## 4: RF (level 2) with GPS points after rough validation

```{r}
filtered_data4 <- data_validation %>%
  # Select only GPS points
  dplyr::filter(Lctnmth == "Location with GPS" |
                  Lctnmth == "Location with differential GPS") %>%
  mutate(EUNISa_1 = as.factor(EUNISa_1), EUNISa_2 = as.factor(EUNISa_2)) %>%
  # Remove all rows with wrong values of indices (not between -1 and 1)
  dplyr::filter(EVI_max <= 1 & EVI_min >= -1) %>%
  dplyr::filter(NDVI_max <= 1) %>%
  dplyr::filter(NDMI_max <= 1) %>%
  dplyr::filter(NDWI_min >= -1) %>%
  # Remove rows with missing values in predictors
  dplyr::filter(if_all(all_of(vars_RF), ~ !is.na(.))) %>%
  # Keep only rows with differences > 0
  dplyr::filter(if_all(contains("diff"), ~ .x > 0)) %>%
  # Filter out points that have not passed the rough validation
  filter(valid_1 == "No rules broken so far") %>%
  # Filter out points where EUNISa_2 is NA
  filter(!is.na(EUNISa_2)) %>%
  # Select only variables needed
  select(PlotObservationID, EUNISa_1, EUNISa_2, all_of(vars_RF))
```

Number of points per category for filtered data:

```{r}
filtered_data4 %>% count(EUNISa_2)
```

```{r}
filtered_data4 <- filtered_data4 %>%
  # Keep only EUNISa_2 categories where there are at least 50 points
  filter(EUNISa_2 %in%
           (filtered_data4 %>% count(EUNISa_2) %>% filter(n >= 50))$EUNISa_2) %>%
  # Drop unused levels of EUNISa_2
  mutate(EUNISa_2 = droplevels(EUNISa_2))
```

Number of points per category for filtered data:

```{r}
filtered_data4 %>% count(EUNISa_2)
```

Split into training and test data sets.

```{r}
set.seed(123)
train_indices4 <- sample(1:nrow(filtered_data4), 0.7 * nrow(filtered_data4))
train_data4 <- filtered_data4[train_indices4, ]
test_data4 <- filtered_data4[-train_indices4, ]
```

```{r eval=FALSE, include=FALSE}
rf4_S2 <- run_rf(vars_RF, train_data4, "EUNISa_2")
print(rf4_S2$time)
save(rf4_S2, file = "objects/10/rf4_S2.Rdata")
```

```{r eval=FALSE, include=FALSE}
predictions_rf4_S2 <- predict(rf4_S2$model, newdata = test_data4,OOB = TRUE,
                           type = "response")
save(predictions_rf4_S2, file = "objects/10/predictions_rf4_S2.Rdata")
```

Confusion matrix:

```{r}
confusionMatrix(predictions_rf4_S2, test_data4$EUNISa_2)
```

```{r eval=FALSE, include=FALSE}
varimp_rf4_S2 <- compute_varimp(rf4_S2$model, nperm = 100)
save(varimp_rf4_S2, file = "objects/10/varimp_rf4_S2.Rdata")
```

```{r eval=FALSE, include=FALSE}
varimp_rf4_cond_S2 <- compute_varimp_cond(rf4_S2$model, nperm = 100)
print(varimp_rf4_cond_S2$time)
save(varimp_rf4_cond_S2, file = "objects/10/varimp_rf4_cond_S2.Rdata")
```

Variable Importance Plot

```{r}
plot(varimp_rf4_S2$varimp, margin = c(9, 3, 2, 2))
plot(varimp_rf4_cond_S2$varimp, margin = c(9, 3, 2, 2))
```

ROC curves:

```{r eval=FALSE, include=FALSE}
roc_data4_S2 <- compute_roc_level2(rf4_S2$model, test_data4)
save(roc_data4_S2, file = "objects/10/roc_data4_S2.Rdata")
```

```{r}
roc4 <- ggplot(roc_data4_S2$roc, aes(x = FPR, y = TPR, color = Class)) +
  geom_line(size = 1.2) +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(
    title = "Multiclass ROC Curves with AUC",
    x = "False Positive Rate",
    y = "True Positive Rate",
    color = "Class (AUC)"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
roc4
```

## 5: RF with GPS points after rough validation, uncorrelated predictors

```{r}
filtered_data5 <- data_validation %>%
  # Select only GPS points
  dplyr::filter(Lctnmth == "Location with GPS" |
                  Lctnmth == "Location with differential GPS") %>%
  mutate(EUNISa_1 = as.factor(EUNISa_1)) %>%
  # Remove all rows with wrong values of indices (not between -1 and 1)
  dplyr::filter(EVI_max <= 1 & EVI_min >= -1) %>%
  dplyr::filter(NDVI_max <= 1) %>%
  dplyr::filter(NDMI_max <= 1) %>%
  dplyr::filter(NDWI_min >= -1) %>%
  # Remove rows with missing values in predictors
  dplyr::filter(if_all(all_of(vars_RF_uncorr), ~ !is.na(.))) %>%
  # Keep only rows with differences > 0
  dplyr::filter(if_all(contains("diff"), ~ .x > 0)) %>%
  # Filter out points that have not passed the rough validation
  dplyr::filter(valid_1 == "No rules broken so far") %>%
  # Select only variables needed
  dplyr::select(PlotObservationID, EUNISa_1, all_of(vars_RF_uncorr))
```

Split into training and test data sets.

```{r}
set.seed(123)
train_indices5 <- sample(1:nrow(filtered_data5), 0.7 * nrow(filtered_data5))
train_data5 <- filtered_data3[train_indices5, ]
test_data5 <- filtered_data3[-train_indices5, ]
```

Number of points per category for filtered data:

```{r}
filtered_data5 %>% count(EUNISa_1)
```

```{r eval=FALSE, include=FALSE}
rf5_S2 <- run_rf(vars_RF_uncorr, train_data5, "EUNISa_1")
print(rf5_S2$time)
save(rf5_S2, file = "objects/10/rf5_S2.Rdata")
```

```{r eval=FALSE, include=FALSE}
predictions_rf5_S2 <- predict(rf5_S2$model, newdata = test_data5,OOB = TRUE,
                           type = "response")
save(predictions_rf5_S2, file = "objects/10/predictions_rf5_S2.Rdata")
```

Confusion matrix:

```{r}
confusionMatrix(predictions_rf5_S2, test_data5$EUNISa_1)
```

```{r eval=FALSE, include=FALSE}
varimp_rf5_S2 <- compute_varimp(rf5_S2$model, nperm = 100)
save(varimp_rf5_S2, file = "objects/10/varimp_rf5_S2.Rdata")
```

```{r eval=FALSE, include=FALSE}
varimp_rf5_cond_S2 <- compute_varimp_cond(rf5_S2$model, nperm = 100)
print(varimp_rf5_cond_S2$time)
save(varimp_rf5_cond_S2, file = "objects/10/varimp_rf5_cond_S2.Rdata")
```

Variable Importance Plot

```{r}
plot(varimp_rf5_S2$varimp, margin = c(9, 3, 2, 2))
plot(varimp_rf5_cond_S2$varimp, margin = c(9, 3, 2, 2))
```

ROC curves:

```{r eval=FALSE, include=FALSE}
roc_data5_S2 <- compute_roc_level1(rf5_S2$model, test_data5)
save(roc_data5_S2, file = "objects/10/roc_data5_S2.Rdata")
```

```{r}
roc5 <- ggplot(roc_data5_S2$roc, aes(x = FPR, y = TPR, color = Class)) +
  geom_line(size = 1.2) +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(
    title = "Multiclass ROC Curves with AUC",
    x = "False Positive Rate",
    y = "True Positive Rate",
    color = "Class (AUC)"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
roc5
```

# RF mmodels after training data refinement

## 6: GPS points, within 10-90th percentile

Refinement based on the variables: SAVI_pos_value, NDWI_min, NDMI_min, NDMI_max (later check variable importances well!).

Distribution plots:

```{r}
distr_plot_percentiles <- function(data, y_vars, y_labels) {
  for (i in seq_along(y_vars)) {
    y_var <- y_vars[[i]]
    y_label <- y_labels[[i]]
    
    # Calculate percentiles per EUNISa_1 group
    percentiles <- data %>%
      group_by(EUNISa_1) %>%
      summarise(
        p10 = quantile(.data[[y_var]], 0.1, na.rm = TRUE),
        p90 = quantile(.data[[y_var]], 0.9, na.rm = TRUE),
        .groups = "drop"
      )
    
    # Join percentiles back to data
    data_flagged <- data %>%
      left_join(percentiles, by = "EUNISa_1") %>%
      mutate(outlier_flag = case_when(
        .data[[y_var]] < p10 ~ "low",
        .data[[y_var]] > p90 ~ "high",
        TRUE ~ "mid"
      ))
    
    # Filter and plot
    p <- ggplot(data = data_flagged %>%
                  filter(EUNISa_1 %in% c("T", "R", "S", "Q")),
                aes(x = EUNISa_1_descr, y = .data[[y_var]])) +
      geom_flat_violin(aes(fill = EUNISa_1_descr),
                       position = position_nudge(x = 0.2, y = 0), alpha = 0.8) +
      geom_point(aes(color = ifelse(outlier_flag == "mid",
                                    EUNISa_1_descr, "grey")),
                 position = position_jitter(width = 0.15), size = 1,
                 alpha = 0.6) +
      geom_boxplot(aes(fill = EUNISa_1_descr), width = 0.2, outlier.shape = NA,
                   alpha = 0.5) +
      stat_summary(fun = mean, geom = "point", shape = 20, size = 1) +
      stat_summary(fun.data = function(x) data.frame(y = max(x, na.rm = TRUE) +
                                                       0.1, label = length(x)),
                   geom = "text", aes(label = ..label..), vjust = 0.5) +
      labs(y = y_label, x = "EUNIS level 1") +
      scale_x_discrete(labels = function(x) str_wrap(x, width = 15)) +
      guides(fill = FALSE, color = FALSE) +
      theme_bw() + coord_flip()
    
    print(p)
  }
}
```

```{r}
distr_plot_percentiles(data_validation %>%
                         # Get GPS points after rough validation
             filter(PlotObservationID %in% filtered_data3$PlotObservationID),
           c("canopy_height", "SAVI_pos_value", "NDWI_min", "NDMI_min", 
             "NDMI_max"),
           c("canopy_height", "SAVI_pos_value", "NDWI_min", "NDMI_min", 
             "NDMI_max"))
```

So far not using canopy_height for refinement.

Calculate percentiles:

```{r}
percentiles6 <- data_validation %>%
  # Get GPS points after rough validation
  filter(PlotObservationID %in% filtered_data3$PlotObservationID) %>%
  group_by(EUNISa_1) %>%
  summarize(
    percentile_10_SAVI_pos_value = quantile(SAVI_pos_value, 
                                            probs = 0.10, na.rm = T),
    percentile_20_SAVI_pos_value = quantile(SAVI_pos_value, 
                                            probs = 0.20, na.rm = T),
    percentile_80_SAVI_pos_value = quantile(SAVI_pos_value, 
                                            probs = 0.80, na.rm = T),
    percentile_90_SAVI_pos_value = quantile(SAVI_pos_value,
                                            probs = 0.90, na.rm = T),
    percentile_10_NDWI_min = quantile(NDWI_min, 
                                            probs = 0.10, na.rm = T),
    percentile_20_NDWI_min = quantile(NDWI_min, 
                                            probs = 0.20, na.rm = T),
    percentile_80_NDWI_min = quantile(NDWI_min, 
                                            probs = 0.80, na.rm = T),
    percentile_90_NDWI_min = quantile(NDWI_min,
                                            probs = 0.90, na.rm = T),
    percentile_10_NDMI_min = quantile(NDMI_min, 
                                            probs = 0.10, na.rm = T),
    percentile_20_NDMI_min = quantile(NDMI_min, 
                                            probs = 0.20, na.rm = T),
    percentile_80_NDMI_min = quantile(NDMI_min, 
                                            probs = 0.80, na.rm = T),
    percentile_90_NDMI_min = quantile(NDMI_min,
                                            probs = 0.90, na.rm = T),
    percentile_10_NDMI_max = quantile(NDMI_max, 
                                            probs = 0.10, na.rm = T),
    percentile_20_NDMI_max = quantile(NDMI_max, 
                                            probs = 0.20, na.rm = T),
    percentile_80_NDMI_max = quantile(NDMI_max, 
                                            probs = 0.80, na.rm = T),
    percentile_90_NDMI_max = quantile(NDMI_max,
                                            probs = 0.90, na.rm = T)
            )
```

```{r}
filtered_data6 <- data_validation %>%
  # Get GPS points after rough validation
  filter(PlotObservationID %in% filtered_data3$PlotObservationID) %>%
  select(PlotObservationID, EUNISa_1, all_of(vars_RF)) %>%
  left_join(percentiles6, by = "EUNISa_1") %>%
  mutate(EUNISa_1 = as.factor(EUNISa_1)) %>%
  filter(
    (SAVI_pos_value >= percentile_10_SAVI_pos_value &
       SAVI_pos_value <= percentile_90_SAVI_pos_value) &
      (NDWI_min >= percentile_10_NDWI_min &
         NDWI_min <= percentile_90_NDWI_min) &
      (NDMI_min >= percentile_10_NDMI_min &
         NDMI_min <= percentile_90_NDMI_min) &
      (NDMI_max >= percentile_10_NDMI_max &
         NDMI_max <= percentile_90_NDMI_max)
    )
```

Split into training and test data sets.

```{r}
set.seed(123)
train_indices6 <- sample(1:nrow(filtered_data6), 0.7 * nrow(filtered_data6))
train_data6 <- filtered_data6[train_indices6, ]
test_data6 <- filtered_data6[-train_indices6, ]
```

Number of points per category for filtered data:

```{r}
filtered_data6 %>% count(EUNISa_1)
```

```{r eval=FALSE, include=FALSE}
rf6_S2 <- run_rf(vars_RF, train_data6, "EUNISa_1")
print(rf6_S2$time)
save(rf6_S2, file = "objects/10/rf6_S2.Rdata")
```

```{r eval=FALSE, include=FALSE}
predictions_rf6_S2 <- predict(rf6_S2$model, newdata = test_data6,OOB = TRUE,
                           type = "response")
save(predictions_rf6_S2, file = "objects/10/predictions_rf6_S2.Rdata")
```

Confusion matrix:

```{r}
confusionMatrix(predictions_rf6_S2, test_data6$EUNISa_1)
```

```{r eval=FALSE, include=FALSE}
varimp_rf6_S2 <- compute_varimp(rf6_S2$model, nperm = 100)
save(varimp_rf6_S2, file = "objects/10/varimp_rf6_S2.Rdata")
```

```{r eval=FALSE, include=FALSE}
varimp_rf6_cond_S2 <- compute_varimp_cond(rf6_S2$model, nperm = 100)
print(varimp_rf6_cond_S2$time)
save(varimp_rf6_cond_S2, file = "objects/10/varimp_rf6_cond_S2.Rdata")
```

Variable Importance Plot

```{r}
plot(varimp_rf6_S2$varimp, margin = c(9, 3, 2, 2))
plot(varimp_rf6_cond_S2$varimp, margin = c(9, 3, 2, 2))
```

ROC curves:

```{r eval=FALSE, include=FALSE}
roc_data6_S2 <- compute_roc_level1(rf6_S2$model, test_data6)
save(roc_data6_S2, file = "objects/10/roc_data6_S2.Rdata")
```

```{r}
roc6 <- ggplot(roc_data6_S2$roc, aes(x = FPR, y = TPR, color = Class)) +
  geom_line(size = 1.2) +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(
    title = "Multiclass ROC Curves with AUC",
    x = "False Positive Rate",
    y = "True Positive Rate",
    color = "Class (AUC)"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
roc6
```

## 7: GPS points, within 20-80th percentile

```{r}
filtered_data7 <- data_validation %>%
  # Get GPS points after rough validation
  filter(PlotObservationID %in% filtered_data3$PlotObservationID) %>%
  select(PlotObservationID, EUNISa_1, all_of(vars_RF)) %>%
  left_join(percentiles6, by = "EUNISa_1") %>%
  mutate(EUNISa_1 = as.factor(EUNISa_1)) %>%
  filter(
    (SAVI_pos_value >= percentile_20_SAVI_pos_value &
       SAVI_pos_value <= percentile_80_SAVI_pos_value) &
      (NDWI_min >= percentile_20_NDWI_min &
         NDWI_min <= percentile_80_NDWI_min) &
      (NDMI_min >= percentile_20_NDMI_min &
         NDMI_min <= percentile_80_NDMI_min) &
      (NDMI_max >= percentile_20_NDMI_max &
         NDMI_max <= percentile_80_NDMI_max)
    )
```

Split into training and test data sets.

```{r}
set.seed(123)
train_indices7 <- sample(1:nrow(filtered_data7), 0.7 * nrow(filtered_data7))
train_data7 <- filtered_data7[train_indices7, ]
test_data7 <- filtered_data7[-train_indices7, ]
```

Number of points per category for filtered data:

```{r}
filtered_data7 %>% count(EUNISa_1)
```

```{r eval=FALSE, include=FALSE}
rf7_S2 <- run_rf(vars_RF, train_data7, "EUNISa_1")
print(rf7_S2$time)
save(rf7_S2, file = "objects/10/rf7_S2.Rdata")
```

```{r eval=FALSE, include=FALSE}
predictions_rf7_S2 <- predict(rf7_S2$model, newdata = test_data7,OOB = TRUE,
                           type = "response")
save(predictions_rf7_S2, file = "objects/10/predictions_rf7_S2.Rdata")
```

Confusion matrix:

```{r}
confusionMatrix(predictions_rf7_S2, test_data7$EUNISa_1)
```

```{r eval=FALSE, include=FALSE}
varimp_rf7_S2 <- compute_varimp(rf7_S2$model, nperm = 100)
save(varimp_rf7_S2, file = "objects/10/varimp_rf7_S2.Rdata")
```

```{r eval=FALSE, include=FALSE}
varimp_rf7_cond_S2 <- compute_varimp_cond(rf7_S2$model, nperm = 100)
print(varimp_rf7_cond_S2$time)
save(varimp_rf7_cond_S2, file = "objects/10/varimp_rf7_cond_S2.Rdata")
```

Variable Importance Plot

```{r}
plot(varimp_rf7_S2$varimp, margin = c(9, 3, 2, 2))
plot(varimp_rf7_cond_S2$varimp, margin = c(9, 3, 2, 2))
```

ROC curves:

```{r eval=FALSE, include=FALSE}
roc_data7_S2 <- compute_roc_level1(rf7_S2$model, test_data7)
save(roc_data7_S2, file = "objects/10/roc_data7_S2.Rdata")
```

```{r}
roc7 <- ggplot(roc_data7_S2$roc, aes(x = FPR, y = TPR, color = Class)) +
  geom_line(size = 1.2) +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(
    title = "Multiclass ROC Curves with AUC",
    x = "False Positive Rate",
    y = "True Positive Rate",
    color = "Class (AUC)"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
roc6
```




# OLD from here

## Maps

### Points GPS

```{r}
# Load world boundaries
world <- ne_countries(scale = "medium", returnclass = "sf")

# Calculate the extent of the points
points_GPS_extent <- data_validation %>%
  dplyr::filter(EUNISa_1 %in% c("T", "R", "S", "Q")) %>%
  dplyr::filter(S2_data == T | Landsat_data == T ) %>%
  dplyr::filter(`Location method` == "Location with differential GPS" |
           `Location method` == "Location with GPS") %>%
  summarise(lon_min = min(Lon_updated, na.rm = TRUE),
            lon_max = max(Lon_updated, na.rm = TRUE),
            lat_min = min(Lat_updated, na.rm = TRUE),
            lat_max = max(Lat_updated, na.rm = TRUE))

# Add padding to the extent (adjust as needed)
padding <- 2  # Adjust padding to your preference
x_limits <- c(points_GPS_extent$lon_min - padding,
              points_GPS_extent$lon_max + padding)
y_limits <- c(points_GPS_extent$lat_min - padding,
              points_GPS_extent$lat_max + padding)

# Create the zoomed map
ggplot() +
  geom_sf(data = world, fill = "lightblue", color = "gray") +
  geom_point(data = data_validation %>%
               dplyr::filter(EUNISa_1 %in% c("T", "R", "S", "Q")) %>%
               dplyr::filter(S2_data == T | Landsat_data == T ) %>%
               dplyr::filter(`Location method` == "Location with differential GPS" |
           `Location method` == "Location with GPS"),
             aes(x = Lon_updated, y = Lat_updated, color = EUNISa_1),
             size = 1) +
  coord_sf(xlim = x_limits, ylim = y_limits) +
  theme_minimal()
```

Number of GPS points by Country:

```{r}
data_validation %>%
  dplyr::filter(EUNISa_1 %in% c("T", "R", "S", "Q")) %>%
  dplyr::filter(S2_data == T | Landsat_data == T ) %>%
  dplyr::filter(`Location method` == "Location with differential GPS" |
           `Location method` == "Location with GPS") %>%
  count(Country)
```

### Points ReSurvey

```{r}
# Calculate the extent of the points
points_resurvey_extent <- data_validation %>%
  dplyr::filter(EUNISa_1 %in% c("T", "R", "S", "Q")) %>%
  dplyr::filter(S2_data == T | Landsat_data == T ) %>%
  summarise(lon_min = min(Lon_updated, na.rm = TRUE),
            lon_max = max(Lon_updated, na.rm = TRUE),
            lat_min = min(Lat_updated, na.rm = TRUE),
            lat_max = max(Lat_updated, na.rm = TRUE))

# Add padding to the extent (adjust as needed)
padding <- 2  # Adjust padding to your preference
x_limits <- c(points_resurvey_extent$lon_min - padding,
              points_resurvey_extent$lon_max + padding)
y_limits <- c(points_resurvey_extent$lat_min - padding,
              points_resurvey_extent$lat_max + padding)

# Create the zoomed map
ggplot() +
  geom_sf(data = world, fill = "lightblue", color = "gray") +
  geom_point(data = data_validation %>%
               dplyr::filter(EUNISa_1 %in% c("T", "R", "S", "Q")) %>%
               dplyr::filter(S2_data == T | Landsat_data == T ),
             aes(x = Lon_updated, y = Lat_updated, color = EUNISa_1),
             size = 1) +
  coord_sf(xlim = x_limits, ylim = y_limits) +
  theme_minimal()
```

Number of ReSurvey points by Country:

```{r}
data_validation %>%
  dplyr::filter(EUNISa_1 %in% c("T", "R", "S", "Q")) %>%
  dplyr::filter(S2_data == T | Landsat_data == T ) %>%
  count(Country)
```

## Cordillera data

```{r}
AlpineGrasslands_indices <- read_csv(
  "C:/Data/MOTIVATE/Cordillera/AlpineGrasslands/AlpineGrassland_Sentinel_Plot_Allyear_Allmetrics.csv")
AlpineGrasslands_phen <- read_csv(
  "C:/Data/MOTIVATE/Cordillera/AlpineGrasslands/AlpineGrasslands_Phenology_SOS_EOS_Peak_NDVI_Amplitude.csv")
AlpineGrasslands_CH <- read_csv(
  "C:/Data/MOTIVATE/Cordillera/AlpineGrasslands/AlpineGrasslands_CanopyHeight_1m.csv")
VegetationTypes_indices <- read_csv(
  "C:/Data/MOTIVATE/Cordillera/VegetationTypes/VegetationTypes_Sentinel_Plot_AllYear_Allmetrics.csv")
VegetationTypes_phen <- read_csv(
  "C:/Data/MOTIVATE/Cordillera/VegetationTypes/VegetationTypes_Phenology_SOS_EOS_Peak_NDVI_Amplitude.csv")
VegetationTypes_CH <- read_csv(
  "C:/Data/MOTIVATE/Cordillera/VegetationTypes/VegetationTypes_CanopyHeight_1m.csv")
```

```{r}
AlpineGrasslands <- AlpineGrasslands_indices %>%
  select(-`system:index`, -.geo, -Localidad) %>%
  rename(H√°bitat = "HÔøΩbitat") %>% 
  full_join(AlpineGrasslands_phen  %>%
              select(-`system:index`, -.geo, -Localidad) %>%
              rename(H√°bitat = "HÔøΩbitat")) %>%
  full_join(AlpineGrasslands_CH  %>%
              select(-`system:index`, -.geo, -Localidad)) %>%
  select(-Date__year, - `PrecisiÔøΩn`) %>%
  mutate(DATE = ymd(DATE)) %>%
  rename(ID = "Releve_num") %>%
  mutate(ID = as.character(ID)) %>%
  mutate(layer = "AlpineGrasslands")
```

```{r}
VegetationTypes <- VegetationTypes_indices %>%
  select(-`system:index`, -.geo) %>%
  full_join(VegetationTypes_phen  %>%
              select(-`system:index`, -.geo)) %>%
  full_join(VegetationTypes_CH  %>%
              select(-`system:index`, -.geo)) %>%
  rename(H√°bitat = "TYPE") %>%
  mutate(layer = "VegetationTypes")
```

Merge both datasets:

```{r}
cordillera <- bind_rows(
  AlpineGrasslands %>% select(DATE, ID, starts_with("NDMI"),
                              starts_with("NDVI"), H√°bitat, "EOS_DOY",
                              "Peak_DOY", "SOS_DOY", "Season_Length",
                              "canopy_height", "layer"),
  VegetationTypes %>% select(DATE, ID, starts_with("NDMI"),
                              starts_with("NDVI"), H√°bitat, "EOS_DOY",
                              "Peak_DOY", "SOS_DOY", "Season_Length",
                              "canopy_height", "layer")
  ) %>%
  mutate(EUNISa_1 = case_when(
    H√°bitat = str_detect(H√°bitat, "Pastizal|Cervunal|grassland|meadow") ~ "R",
    H√°bitat = str_detect(H√°bitat, "forest") ~ "T",
    H√°bitat = str_detect(H√°bitat, "Scrub|scrub|Shrubland|shrubland|shrub|Heathland") ~ "S",
    H√°bitat = str_detect(H√°bitat, "Suelo|Scree|scree|cliff") ~ "U",
    H√°bitat = is.na(H√°bitat) ~ "R",
    TRUE ~ NA_character_),
    EUNISa_1_descr = case_when(
      EUNISa_1 == "R" ~ "Grasslands",
      EUNISa_1 == "T" ~ "Forests and other wooded land",
      EUNISa_1 == "S" ~ "Heathlands, scrub and tundra",
      EUNISa_1 == "U" ~ "Inland habitats with no or little soil")
    )
```

### NDVI, NDMI

```{r}
distr_plot(cordillera,
           c("NDVI_max", "NDVI_p90", "NDVI_min", "NDVI_p10"), 
           c("NDVI max", "NDVI p90", "NDVI min", "NDVI p10"))
distr_plot(cordillera,
           c("NDMI_max", "NDMI_p90", "NDMI_min", "NDMI_p10"), 
           c("NDMI max", "NDMI p90", "NDMI min", "NDMI p10"))
```

# Session info

```{r}
sessionInfo()
```

